{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from tsv to csv\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "\n",
    "file_name = \"messer_SHC_note_final_2.tsv\"    \n",
    "out_file = open(\"SHC_note2.csv\",'w')\n",
    "i=0\n",
    "with open(file_name,'rb') as tsvin, open('new.csv', 'wb') as csvout:\n",
    "    tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "    csvout = csv.writer(csvout)\n",
    "\n",
    "    for row in tsvin:\n",
    "        #print(type(row))\n",
    "        if len(row) > 0:\n",
    "            csvout.writerow(row)\n",
    "        #i += 1\n",
    "        #if i > 3:\n",
    "        #    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#get rid of progress notes and others\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_RAW.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_FILTERED.csv\",'w')\n",
    "i=1\n",
    "        \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    outfieldnames = reader.fieldnames \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        if (line['Note'] is None) or (len(line['Note']) == 0):\n",
    "            print(\"malformed at %s\", i)\n",
    "            continue\n",
    "        notebody = line['Note'].lower()\n",
    "        notetype = line['Note_type_Desc'].lower()\n",
    "        if \"EEG Number\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"standard international system 10-20\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"standard international 10-20\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"conditions of recording\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"conditions of the recording\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"eeg #\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "            \n",
    "        if \"neurodiagnostic\" in notetype:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"outpatient procedure\" in notetype:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + notebody[:100], i )\n",
    "        #print()\n",
    "        #if i > 1900:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #i += 1\n",
    "    #if (i>600):\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# normal versus abn\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_FILTERED.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_IMPRESS.csv\",'w')\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation:\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegno = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:|Impression|impression:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:|SUMMARY|Summary)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegnoLoose = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "\n",
    "\n",
    "def findTrueImpression(eeg_no,line,i):\n",
    "    wordlist = eeg_no.lower().split()\n",
    "    words = [''.join(c for c in s if c not in string.punctuation) for s in wordlist]\n",
    "    #print(words)\n",
    "    if \"normal\" in words and \"abnormal\" in words:\n",
    "        #equivocal\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i) + \"EQUIVOCAL\"+  eeg_no)\n",
    "            print()\n",
    "            \n",
    "    elif \"abnormal\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"status\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"seizure\" in words and \"nonepileptic\" in words:\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i)  +\"UNKNOWN-\"+ eeg_no)\n",
    "            print()\n",
    "    elif \"nonepileptic\" in words:\n",
    "            line['impression'] = \"normal\"\n",
    "    elif \"seizure\" in words:\n",
    "            line['impression'] = \"abnormal\"\n",
    "    elif \"slowing\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"normal\" in words:\n",
    "        line['impression'] = \"normal\"\n",
    "    else: \n",
    "        line['impression'] = eeg_no\n",
    "    #print(\"(\" + repr(i) + \")\" + line['impression'] + \"-\" + eeg_no.lower())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def findSpecificPhrases(eeg_no,line,i):\n",
    "    if \"This EEG is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"\n",
    "    elif \"is normal for age\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the broad normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG recording was normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This record is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"  \n",
    "    elif \"EEG recording is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This is a normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "         \n",
    "\n",
    "    elif \"This EEG is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"           \n",
    "    elif \"is abnormal because of\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"     \n",
    "    elif \"recording is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"markedly abnormal\" in eeg_no.lower():\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"is abnormal due to\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This record is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This is an abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "        \n",
    "\n",
    "    else:\n",
    "        return False        \n",
    "\n",
    "        \n",
    "i=1        \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    outfieldnames = reader.fieldnames \n",
    "    outfieldnames.append('impression')     \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            findTrueImpression(eeg_no,line,i)\n",
    "        else:\n",
    "            #try a looser find that may introduce more false information (more sensitive less specific pattern)\n",
    "            m = re_eegnoLoose.search(line['note'])\n",
    "            if m:\n",
    "                eeg_no = m.group('eegno')\n",
    "                findTrueImpression(eeg_no,line,i)\n",
    "            elif \"preliminary\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            elif \"prelim\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            else:\n",
    "                print(\"###IMPRESSION BLOCK MISSING-\" + repr(i) + line['note'])\n",
    "                print()\n",
    "\n",
    "        writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + notebody[:100], i )\n",
    "        #print()\n",
    "        #if i > 4446:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import csvkit as csv\n",
    "\n",
    "allReports = \"\"\n",
    "with open(\"SHC_ALL_NOTES_FILTERED.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        allReports += line['note'] + \" \\n\\n\"\n",
    "        # print(desc)\n",
    "        # now manipulate the note body\n",
    "        \n",
    "f = open(\"allReports.txt\", \"w\")\n",
    "f.write(allReports)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a histogram of the most commonly used words in all EEG reports\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = open(\"allReports.txt\", \"r\")\n",
    "allReports = f.read() \n",
    "f.close()\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "allwords_stemmed = tokenize_and_stem(allReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEuCAYAAAByL06RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3876129e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x380415e10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get rid of stop words\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_words = [word for word in allwords_stemmed if word not in stopwords]\n",
    "\n",
    "letter_counts = Counter(filtered_words)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df = df.sort_values(by=0, ascending=0)\n",
    "df1 = df[:20]\n",
    "plt.show(block=True)\n",
    "df1.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epileptiform discharges; standard international; electrode placements;\n",
      "identifying information; stanford hospital; technical notes; possible\n",
      "seizures; technically adequate; report identifying; movement leads;\n",
      "referring dept; eye movement; ekg lead; test dates; adequate\n",
      "activation; administered alertness; sensory stimulation; diffuse\n",
      "slowing; single channel; digital nihon-kohden\n"
     ]
    }
   ],
   "source": [
    "# most common bigrams\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "report = allReports.lower()\n",
    "reportNLTK = nltk.Text(nltk.tokenize.word_tokenize(report))\n",
    "reportNLTK.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seizure . The record was continuous , and an EKG lead . The record was "
     ]
    }
   ],
   "source": [
    "# generate some eeg reports for me by sequencing bigrams\n",
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "#print(type(text))\n",
    "corpus = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"allReports.txt\")\n",
    "words=corpus.words()\n",
    "#get rid of stop words\n",
    "#filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "#words = filtered_words\n",
    "\n",
    "#print(type(words))\n",
    "bigrams = nltk.bigrams(words)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "generate_model(cfd, 'seizure')\n",
    "#filtered_words = [word for word in word_list if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epileptic events and an EKG lead . The record was continuous , and an EKG \n",
      "slow activity . The record was continuous , and an EKG lead . The record \n",
      "discharge . The record was continuous , and an EKG lead . The record was \n",
      "periodic discharges . The record was continuous , and an EKG lead . The record \n",
      "impression . The record was continuous , and an EKG lead . The record was \n",
      "background consisted of the EEG machine , and an EKG lead . The record was \n",
      "infantile spasms , and an EKG lead . The record was continuous , and an \n",
      "neonatal seizures . The record was continuous , and an EKG lead . The record \n",
      "seizure . The record was continuous , and an EKG lead . The record was \n",
      "seizures . The record was continuous , and an EKG lead . The record was \n",
      "This is a digital Nihon - 20 electrode placements , and an EKG lead . \n",
      "electrographic seizures . The record was continuous , and an EKG lead . The record \n",
      "indicates cerebral dysfunction from a digital Nihon - 20 electrode placements , and an EKG \n",
      "represent a digital Nihon - 20 electrode placements , and an EKG lead . The \n",
      "generalized tonic - 20 electrode placements , and an EKG lead . The record was \n",
      "slowing . The record was continuous , and an EKG lead . The record was \n",
      "focal slowing . The record was continuous , and an EKG lead . The record \n",
      "3 / s spindles . The record was continuous , and an EKG lead . \n",
      "hz . The record was continuous , and an EKG lead . The record was \n",
      "second which was continuous , and an EKG lead . The record was continuous , \n",
      "button . The record was continuous , and an EKG lead . The record was \n",
      "cry and an EKG lead . The record was continuous , and an EKG lead \n",
      "clonic seizure . The record was continuous , and an EKG lead . The record \n",
      "tonic - 20 electrode placements , and an EKG lead . The record was continuous \n",
      "partial seizures . The record was continuous , and an EKG lead . The record \n",
      "sleep ( TYLENOL ) tablet 1 - 20 electrode placements , and an EKG lead \n",
      "vertex waves , and an EKG lead . The record was continuous , and an \n",
      "temporal region . The record was continuous , and an EKG lead . The record \n",
      "frontal region . The record was continuous , and an EKG lead . The record \n",
      "occipital sharp waves , and an EKG lead . The record was continuous , and \n",
      "alpha and an EKG lead . The record was continuous , and an EKG lead \n",
      "beta activity . The record was continuous , and an EKG lead . The record \n",
      "delta activity . The record was continuous , and an EKG lead . The record \n",
      "theta activity . The record was continuous , and an EKG lead . The record \n",
      "sudden onset seizures . The record was continuous , and an EKG lead . The \n",
      "waxing and an EKG lead . The record was continuous , and an EKG lead \n",
      "waning mental status epilepticus . The record was continuous , and an EKG lead . "
     ]
    }
   ],
   "source": [
    "generate_model(cfd, 'epileptic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'slow')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'discharge')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'periodic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'impression')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'background')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'infantile')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'neonatal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'seizure')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'seizures')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'This')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'electrographic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'indicates')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'represent')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'generalized')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'slowing')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'focal')\n",
    "print(\"\")\n",
    "generate_model(cfd, '3')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'hz')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'second')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'button')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'cry')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'clonic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'tonic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'partial')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'sleep')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'vertex')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'temporal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'frontal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'occipital')\n",
    "print(\"\")\n",
    "#generate_model(cfd, 'hypsarrythmia')\n",
    "#print(\"\")\n",
    "generate_model(cfd, 'alpha')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'beta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'delta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'theta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'sudden')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'waxing')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'waning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fraction of words in text that are not in the stopword list\n",
    "def content_fraction(text):\n",
    "     stopwords = nltk.corpus.stopwords.words('english')\n",
    "     content = [w for w in text if w.lower() not in stopwords]\n",
    "     return len(content) / len(text)\n",
    "\n",
    "content_fraction(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#tutorial example of classification\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#tutorial continued\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#tutorial\n",
    "#what does it containt \n",
    "movie_reviews.categories()\n",
    "movie_reviews.fileids('neg')\n",
    "movie_reviews.categories()\n",
    "i = 1\n",
    "for doc in documents:\n",
    "    print(str(i) + \": \"+ doc[1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
    "print(featuresets[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "exclude = set(string.punctuation)\n",
    "print(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the most common word associated with the interpretation result (abnormal versus normal)\n",
    "#should take 10 mins\n",
    "import random\n",
    "import string\n",
    "documents = list()\n",
    "notebody = list()\n",
    "result = list()\n",
    "exclude = set(string.punctuation)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "with open(\"SHC_ALL_NOTES_IMPRESS.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        notebody = line['note'].lower()\n",
    "        notebodyStemmed = tokenize_and_stem(notebody)        \n",
    "        filtered_words = [word for word in notebodyStemmed if word not in stopwords]\n",
    "\n",
    "        #notebody = ''.join(ch for ch in notebody if ch not in exclude)\n",
    "        #notebody = notebody.split() \n",
    "        #notebody = [word for word in notebody if word not in stopwords.words('english')]\n",
    "        category = line['impression']\n",
    "        if (category == 'abnormal' or category == 'normal'):\n",
    "            documents.append((list(filtered_words), category))\n",
    "\n",
    "#documents[1]\n",
    "        # print(desc)\n",
    "        # now manipulate the note body\n",
    "\n",
    "random.shuffle(documents)\n",
    "#documents[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "i = 1\n",
    "for doc in documents:\n",
    "    print(doc[0])\n",
    "    i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructing a list of the 2000 most frequent words in the overall corpus\n",
    "import nltk\n",
    "words = list()\n",
    "for doc in documents:\n",
    "    words += doc[0] \n",
    "    \n",
    "#print(words)    \n",
    "all_words = nltk.FreqDist(w for w in words)\n",
    "word_features = list(all_words)[:4000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "for (d,c) in documents:\n",
    "    print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807882804325\n",
      "Most Informative Features\n",
      "contains(stimulus-induc) = True           abnorm : normal =     41.8 : 1.0\n",
      " contains(norepinephrin) = True           abnorm : normal =     34.9 : 1.0\n",
      " contains(centro-tempor) = True           abnorm : normal =     21.5 : 1.0\n",
      "    contains(maxalt-mlt) = True           normal : abnorm =     17.9 : 1.0\n",
      "      contains(accommod) = True           normal : abnorm =     16.2 : 1.0\n",
      "      contains(cigarett) = True           normal : abnorm =     13.8 : 1.0\n",
      "       contains(letharg) = True           abnorm : normal =     13.4 : 1.0\n",
      "       contains(ela-max) = True           abnorm : normal =     12.8 : 1.0\n",
      "           contains(emt) = True           normal : abnorm =     12.8 : 1.0\n",
      " contains(desvenlafaxin) = True           normal : abnorm =     12.8 : 1.0\n",
      "       contains(cathflo) = True           abnorm : normal =     12.5 : 1.0\n",
      "     contains(nonspecif) = True           abnorm : normal =     12.1 : 1.0\n",
      "contains(temporal/centr) = True           abnorm : normal =     11.1 : 1.0\n",
      "         contains(evacu) = True           abnorm : normal =     10.6 : 1.0\n",
      "  contains(neurosurgeri) = True           abnorm : normal =     10.5 : 1.0\n",
      " contains(temporopariet) = True           abnorm : normal =      9.5 : 1.0\n",
      "       contains(min:152) = True           normal : abnorm =      9.4 : 1.0\n",
      "        contains(prelud) = True           normal : abnorm =      9.4 : 1.0\n",
      "          contains(54/f) = True           normal : abnorm =      9.4 : 1.0\n",
      "       contains(9-10per) = True           normal : abnorm =      9.4 : 1.0\n",
      "      contains(gaviscon) = True           normal : abnorm =      9.4 : 1.0\n",
      "   contains(epilepticus) = True           abnorm : normal =      9.3 : 1.0\n",
      " contains(frontal-centr) = True           abnorm : normal =      8.9 : 1.0\n",
      "    contains(e258/e258a) = True           abnorm : normal =      8.8 : 1.0\n",
      "       contains(copious) = True           abnorm : normal =      8.5 : 1.0\n",
      "         contains(chain) = True           abnorm : normal =      8.0 : 1.0\n",
      "      contains(ointment) = True           abnorm : normal =      7.9 : 1.0\n",
      "contains(phenergan-codein) = True           normal : abnorm =      7.7 : 1.0\n",
      "     contains(estraderm) = True           normal : abnorm =      7.7 : 1.0\n",
      "     contains(uncoordin) = True           normal : abnorm =      7.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "ccount = int (len(featuresets) / 2) - 1\n",
    "#take half of the total as the training set and the other half as a test set\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[ccount:], featuresets[:ccount]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (d) in documents:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14336\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# in order to load a corpus, reports must be in separate files // don't need to run this again\n",
    "\n",
    "capture_eegno = r'(?:DATE OF SERVICE:|STUDY DATE:|DATE EEG:|Date:|Service Date:|Study date|Study dates|DATE OF EEG:|start|T:|test dates:|Date of study:|Exam date|exam date:)\\s*(?P<eegno>[\\d/-]+)\\s*'\n",
    "capture_eegno1 = r'(?:Date:|Study date |Study date:)\\s*(?P<eegno>(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)\\s*[,\\s\\d/-]+)(?:[a-z]+)'\n",
    "capture_eegno2 = r'(?<!DOB:  )(?P<eegno>[\\d]+/[\\d]+/[\\d]+)' #DOB:  10/04/1993   \n",
    "eegDateRange = r'(?P<eegno>[\\d/]+-[\\d/]+)'\n",
    "eegDateStrict = r'(?P<eegno>[\\d]+/[\\d]+/[\\d]+)'\n",
    "#capture_eegno = r'(?P<eegno>[0-9]+)\\s*'\n",
    "re_eegno = re.compile(capture_eegno, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno1 = re.compile(capture_eegno1, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno2 = re.compile(capture_eegno2, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateRange = re.compile(eegDateRange, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateStrict = re.compile(eegDateStrict, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "\n",
    "out_file = open(\"lpch_eeg_reports_interp_date_impression.csv\",'w')\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    outfieldnames = reader.fieldnames\n",
    "    outfieldnames.append('date')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='***')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for line in reader:\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        print(str(i) + \": \", end='')\n",
    "        #print(str(i) + \": \" + str(type(m)), end='')\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            #print(eeg_no)\n",
    "        else:\n",
    "            m = re_eegno1.search(line['note'])\n",
    "            if m:\n",
    "                eeg_no = m.group('eegno')\n",
    "                #print(eeg_no)\n",
    "            else:\n",
    "                #print(\"!!!!!!!!!!!\")\n",
    "                for match in re.finditer(capture_eegno2,line['note']):\n",
    "                    eeg_no = match.group('eegno')\n",
    "                    \n",
    "                #m = re_eegno2.search(line['note'])\n",
    "                #if m:\n",
    "                #    eeg_no = m.group('eegno')\n",
    "                #print(eeg_no +\"???????\")\n",
    "\n",
    "        #if len(eeg_no) > 12:\n",
    "        #    m = eegDateRange.search(eeg_no)            \n",
    "        #    if m:\n",
    "        #        print(\"bad\")\n",
    "        #        m = eegDateStrict.search(eeg_no)\n",
    "        #        m = eegDateStrict.search(eeg_no)\n",
    "        #        if m:\n",
    "        #            eeg_no = m.group('eegno')\n",
    "        if ((len(eeg_no) < 5) | (len(eeg_no) > 12)):\n",
    "            #print(\"possibly bad: \" + eeg_no)\n",
    "            for match in re.finditer(capture_eegno2,line['note']):\n",
    "                eeg_no = match.group('eegno')\n",
    "        line['date'] = eeg_no \n",
    "        writer.writerow(line)\n",
    "        \n",
    "        #print(eeg_no)    \n",
    "        #i += 1\n",
    "        #if (i>600):\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capture_eegno = r'(?:EEG TYPE:)\\s*(?P<eegno>.+?)(?:history|report|clinical|condition|patient|location)'\n",
    "capture_eegno1 = r'(?:Date:|Study date |Study date:)\\s*(?P<eegno>(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)\\s*[,\\s\\d/-]+)(?:[a-z]+)'\n",
    "capture_eegno2 = r'(?<!DOB:  )(?P<eegno>[\\d]+/[\\d]+/[\\d]+)' #DOB:  10/04/1993   \n",
    "eegDateRange = r'(?P<eegno>[\\d/]+-[\\d/]+)'\n",
    "eegDateStrict = r'(?P<eegno>[\\d]+/[\\d]+/[\\d]+)'\n",
    "#capture_eegno = r'(?P<eegno>[0-9]+)\\s*'\n",
    "re_eegno = re.compile(capture_eegno, re.DOTALL|re.IGNORECASE)\n",
    "re_eegno1 = re.compile(capture_eegno1, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno2 = re.compile(capture_eegno2, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateRange = re.compile(eegDateRange, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateStrict = re.compile(eegDateStrict, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "\n",
    "out_file = open(\"lpch_eeg_reports_interp_date_impression_type.csv\",'w')\n",
    "\n",
    "def detectType(eeg_no):\n",
    "    if re.search(\"ambulatory\", eeg_no, re.IGNORECASE):\n",
    "        return \"amb\"\n",
    "    if re.search(\"routine\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"routine\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"portable\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"spot\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"out\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"sleep\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"continuous\", eeg_no, re.IGNORECASE):\n",
    "        return \"ltm\"\n",
    "    if re.search(\"video\", eeg_no, re.IGNORECASE):\n",
    "        return \"ltm\"\n",
    "    if re.search(\"intraoperative\", eeg_no, re.IGNORECASE):\n",
    "        return \"iom\"\n",
    "    if re.search(\"electrocort\", eeg_no, re.IGNORECASE):\n",
    "        return \"iom\"\n",
    "    if re.search(\"amplitude\", eeg_no, re.IGNORECASE):\n",
    "        return \"aeeg\"\n",
    "    if re.search(\"aeeg\", eeg_no, re.IGNORECASE):\n",
    "        return \"aeeg\"\n",
    "    return \"\"\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_date_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    outfieldnames = reader.fieldnames\n",
    "    outfieldnames.append('type')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='***')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for line in reader:\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        print(str(i) + \": \", end='')\n",
    "        i += 1\n",
    "        #print(str(i) + \": \" + str(type(m)), end='')\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            eeg_no = detectType(eeg_no)\n",
    "            \n",
    "        if (len(eeg_no) == 0):\n",
    "            eeg_no = detectType(line['note'])\n",
    "\n",
    "        line['type'] = eeg_no\n",
    "        \n",
    "        writer.writerow(line)\n",
    "        #if (i>1556): \n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in order to load a corpus, reports must be in separate files\n",
    "from dateutil import parser\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_date_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    \n",
    "    for line in reader:\n",
    "        date_str= line['date']\n",
    "        try:\n",
    "            dateObj = parser.parse(date_str)\n",
    "            #print(dateObj.date())\n",
    "            dateFile = (str(dateObj.date()))\n",
    "            f = open(\"reports/\" + dateFile + \".txt\", \"a+\")\n",
    "            f.write(line['note'])\n",
    "            f.close()\n",
    "        except:\n",
    "            print(\"Unexpected error:\", str(sys.exc_info()[0]) + date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "#fileids = os.listdir(\"reports\")\n",
    "corpus_root = 'reports'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:10])\n",
    "           for fileid in wordlists.fileids()\n",
    "           for w in wordlists.words(fileid)\n",
    "           for target in ['grda', 'gpd']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['grda','gpd'], samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see if there are natural clusters, unsupervised\n",
    "#ngram of word, each one is dimension of word, then \n",
    "#applied text analysis python machine learning approaches on slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try tagging \n",
    "\n",
    "textAll_tokenized = list()\n",
    "with open(\"lpch_eeg_reports_interp_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        #print(line['note'])\n",
    "        text_raw = line['note'].lower()\n",
    "        text_tokenized = nltk.tokenize.word_tokenize(text_raw)\n",
    "        textAll_tokenized += text_tokenized\n",
    "        #words = list(text_raw.split())\n",
    "        #print(words)\n",
    "        #break\n",
    "\n",
    "text = nltk.Text(textAll_tokenized)\n",
    "text_tagged = nltk.pos_tag(text)\n",
    "\n",
    "        #print(text[10:200])\n",
    "print(text.similar('alpha'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which of these tags are the most common\n",
    "\n",
    "Tag\tMeaning\tEnglish Examples\n",
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADP\tadposition\ton, of, at, with, by, into, under\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CONJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "PRON\tpronoun\the, their, her, its, my, I, us\n",
    "VERB\tverb\tis, say, told, given, playing, would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('NN.*')\n",
    "nltk.help.upenn_tagset('JJ.*')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in text_tagged)\n",
    "tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tag_pairs = nltk.bigrams(text_tagged)\n",
    "#i = 0\n",
    "#for (a, b) in word_tag_pairs:\n",
    "#    print(b[1])\n",
    "#    i += 1\n",
    "#    if i>22:\n",
    "#        break\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'JJ']\n",
    "#print(noun_preceders)\n",
    "fdist = nltk.FreqDist(noun_preceders)\n",
    "[tag for (tag, _) in fdist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what are teh most common verbs\n",
    "word_tag_fd = nltk.FreqDist(text_tagged)\n",
    "[wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# most likely words for a given tag VBN = \n",
    "cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in text_tagged)\n",
    "list(cfd2['VBN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find vbd (past tense) and VBN (past participle), find words which can be both and see surrounding text\n",
    "nltk.help.upenn_tagset('VB.*')\n",
    "cfd1 = nltk.ConditionalFreqDist(text_tagged)\n",
    "[w for w in cfd1.conditions() if 'VBD' in cfd1[w] and 'VBN' in cfd1[w]]\n",
    "idx1 = text_tagged.index(('localized', 'VBD'))\n",
    "print(text_tagged[idx1-4:idx1+1])\n",
    "idx2 = text_tagged.index(('localized', 'VBN'))\n",
    "print(text_tagged[idx2-4:idx2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
