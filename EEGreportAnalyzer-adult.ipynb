{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from tsv to csv\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "\n",
    "file_name = \"messer_SHC_note_final_2.tsv\"    \n",
    "out_file = open(\"SHC_note2.csv\",'w')\n",
    "i=0\n",
    "with open(file_name,'rb') as tsvin, open('new.csv', 'wb') as csvout:\n",
    "    tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "    csvout = csv.writer(csvout)\n",
    "\n",
    "    for row in tsvin:\n",
    "        #print(type(row))\n",
    "        if len(row) > 0:\n",
    "            csvout.writerow(row)\n",
    "        #i += 1\n",
    "        #if i > 3:\n",
    "        #    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#get rid of progress notes and others\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_RAW.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_FILTERED.csv\",'w')\n",
    "i=1\n",
    "        \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    outfieldnames = reader.fieldnames \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        if (line['Note'] is None) or (len(line['Note']) == 0):\n",
    "            print(\"malformed at %s\", i)\n",
    "            continue\n",
    "        notebody = line['Note'].lower()\n",
    "        notetype = line['Note_type_Desc'].lower()\n",
    "        if \"EEG Number\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"standard international system 10-20\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"standard international 10-20\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"conditions of recording\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"conditions of the recording\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"eeg #\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "            \n",
    "        if \"neurodiagnostic\" in notetype:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"outpatient procedure\" in notetype:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + notebody[:100], i )\n",
    "        #print()\n",
    "        #if i > 1900:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #i += 1\n",
    "    #if (i>600):\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# normal versus abn\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_FILTERED.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_IMPRESS.csv\",'w')\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation:\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegno = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:|Impression|impression:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:|SUMMARY|Summary)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegnoLoose = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "\n",
    "\n",
    "def findTrueImpression(eeg_no,line,i):\n",
    "    wordlist = eeg_no.lower().split()\n",
    "    words = [''.join(c for c in s if c not in string.punctuation) for s in wordlist]\n",
    "    #print(words)\n",
    "    if \"normal\" in words and \"abnormal\" in words:\n",
    "        #equivocal\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i) + \"EQUIVOCAL\"+  eeg_no)\n",
    "            print()\n",
    "            \n",
    "    elif \"abnormal\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"status\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"seizure\" in words and \"nonepileptic\" in words:\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i)  +\"UNKNOWN-\"+ eeg_no)\n",
    "            print()\n",
    "    elif \"nonepileptic\" in words:\n",
    "            line['impression'] = \"normal\"\n",
    "    elif \"seizure\" in words:\n",
    "            line['impression'] = \"abnormal\"\n",
    "    elif \"slowing\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"normal\" in words:\n",
    "        line['impression'] = \"normal\"\n",
    "    else: \n",
    "        line['impression'] = eeg_no\n",
    "    #print(\"(\" + repr(i) + \")\" + line['impression'] + \"-\" + eeg_no.lower())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def findSpecificPhrases(eeg_no,line,i):\n",
    "    if \"This EEG is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"\n",
    "    elif \"is normal for age\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the broad normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG recording was normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This record is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"  \n",
    "    elif \"EEG recording is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This is a normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "         \n",
    "\n",
    "    elif \"This EEG is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"           \n",
    "    elif \"is abnormal because of\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"     \n",
    "    elif \"recording is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"markedly abnormal\" in eeg_no.lower():\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"is abnormal due to\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This record is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This is an abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "        \n",
    "\n",
    "    else:\n",
    "        return False        \n",
    "\n",
    "        \n",
    "i=1        \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    outfieldnames = reader.fieldnames \n",
    "    outfieldnames.append('impression')     \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            findTrueImpression(eeg_no,line,i)\n",
    "        else:\n",
    "            #try a looser find that may introduce more false information (more sensitive less specific pattern)\n",
    "            m = re_eegnoLoose.search(line['note'])\n",
    "            if m:\n",
    "                eeg_no = m.group('eegno')\n",
    "                findTrueImpression(eeg_no,line,i)\n",
    "            elif \"preliminary\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            elif \"prelim\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            else:\n",
    "                print(\"###IMPRESSION BLOCK MISSING-\" + repr(i) + line['note'])\n",
    "                print()\n",
    "\n",
    "        writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + notebody[:100], i )\n",
    "        #print()\n",
    "        #if i > 4446:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normal versus abn AND remove impression block from notebody\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_FILTERED.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_IMPRESS.csv\",'w')\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation:\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegno = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:|Impression|impression:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:|SUMMARY|Summary)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegnoLoose = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "\n",
    "eegTypePattern = r'[a-z]{1}[0-9]+\\-[0-9]+'\n",
    "re_eegnoType = re.compile(eegTypePattern, re.DOTALL|re.IGNORECASE)\n",
    "\n",
    "eegDurationPattern = r'Duration:\\s*([\\d]+:[\\d]+:[\\d]+)'\n",
    "re_eegnoDuration = re.compile(eegDurationPattern, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern2 = r'Duration of Study:\\s*(([\\d]+)\\s*hour|([\\d]+)\\s*day)'\n",
    "re_eegnoDuration2 = re.compile(eegDurationPattern2, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern3 = r'test dates:\\s*([\\d\\/]+)\\s*-\\s*([\\d\\/]+)' \n",
    "re_eegnoDuration3 = re.compile(eegDurationPattern3, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern4 = r'\\s*([\\d\\.]+)\\s*(hour(s?)|day(s?))' \n",
    "re_eegnoDuration4 = re.compile(eegDurationPattern4, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern5 = r'test dates:\\s*([\\d\\/]+).*?-\\s*([\\d\\/]+)' \n",
    "re_eegnoDuration5 = re.compile(eegDurationPattern5, re.DOTALL|re.IGNORECASE)\n",
    "\n",
    "eegAbnormalityTypePattern = ['no epileptiform', 'absence of epileptiform', 'not epileptiform']\n",
    "#r'(?<!no)\\s+(epileptiform)'\n",
    "eegAbnormalityTypePattern2 = r'(?<!absence of)\\s+(epileptiform)'\n",
    "#(?:(?!\\bno\\b).)*?\\bepileptiform\\b'\n",
    "#re_eegnoAbnormalityType1 = re.compile(eegAbnormalityTypePattern1, re.DOTALL|re.IGNORECASE|re.MULTILINE)\n",
    "\n",
    "def findTrueImpression(eeg_no,line,i):\n",
    "    wordlist = eeg_no.lower().split()\n",
    "    words = [''.join(c for c in s if c not in string.punctuation) for s in wordlist]\n",
    "    #print(words)\n",
    "    if \"normal\" in words and \"abnormal\" in words:\n",
    "        #equivocal\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i) + \"EQUIVOCAL\"+  eeg_no)\n",
    "            print()\n",
    "            \n",
    "    elif \"abnormal\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"status\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"seizure\" in words and \"nonepileptic\" in words:\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i)  +\"UNKNOWN-\"+ eeg_no)\n",
    "            print()\n",
    "    elif \"nonepileptic\" in words:\n",
    "            line['impression'] = \"normal\"\n",
    "    elif \"seizure\" in words:\n",
    "            line['impression'] = \"abnormal\"\n",
    "    elif \"slowing\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"normal\" in words:\n",
    "        line['impression'] = \"normal\"\n",
    "    else: \n",
    "        line['impression'] = eeg_no\n",
    "    #print(\"(\" + repr(i) + \")\" + line['impression'] + \"-\" + eeg_no.lower())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def findSpecificPhrases(eeg_no,line,i):\n",
    "    if \"This EEG is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"\n",
    "    elif \"is normal for age\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the broad normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG recording was normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This record is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"  \n",
    "    elif \"EEG recording is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This is a normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "         \n",
    "\n",
    "    elif \"This EEG is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"           \n",
    "    elif \"is abnormal because of\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"     \n",
    "    elif \"recording is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"markedly abnormal\" in eeg_no.lower():\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"is abnormal due to\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This record is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This is an abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "        \n",
    "\n",
    "    else:\n",
    "        return False        \n",
    "\n",
    "\n",
    "def setImpressionBody(m,line,i):\n",
    "    impstr = line['note'][m.start():]\n",
    "    n = re.search(r'(?:comments|comment|clinical correlation):', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        line['impressionBody'] = impstr[:n.start()]\n",
    "    else:\n",
    "        line['impressionBody'] = impstr\n",
    "\n",
    "    # get rid of double spaces\n",
    "    line['impressionBody'] = \" \".join(line['impressionBody'].split())\n",
    "\n",
    "def setImpressionType(line,i):\n",
    "    line['impressionType'] = \"\"        \n",
    "    impstr = line['impressionBody']\n",
    "    if impstr is None:\n",
    "        return\n",
    "    #https://regex101.com/\n",
    "\n",
    "    n = re.search(r'(?:(?<!absence of)(?<!no)(?<!not))\\s+(epileptiform)', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        if (\"other epileptiform\" not in impstr.lower()):\n",
    "            line['impressionType'] += ' ed' #epileptiform discharge\n",
    "\n",
    "    n = re.search(r'(?:(?<!absence of)(?<!no)(?<!not))\\s+(discharge|spike)', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        line['impressionType'] += ' dc' #discharges which are more questionable\n",
    "        \n",
    "    n = re.search(r'seizure', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        if (\"no evidence for electrographic seizures\" not in impstr.lower()) and \\\n",
    "           (\"for a seizure disorder\" not in impstr.lower()) and \\\n",
    "           (\"no evidence for electrographic seizure\" not in impstr.lower()) and \\\n",
    "           (\"does not exclude a clinical diagnosis of seizure\" not in impstr.lower()) and \\\n",
    "           (\"no electrographic seizure\" not in impstr.lower()):\n",
    "            line['impressionType'] += ' sz'\n",
    "\n",
    "def durationByDates(t1, t2):\n",
    "    dtformat = '%m/%d/%Y'\n",
    "    try:\n",
    "        t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "        t2 = datetime.datetime.strptime(t2, dtformat)\n",
    "        t1 = t1.date()\n",
    "        t2 = t2.date()\n",
    "        td = t2-t1\n",
    "        return (repr(td.days*24) +\":\"+ repr(td.seconds//3600).zfill(2)  +\":\"+ repr((td.seconds//60)%60).zfill(2) )\n",
    "    except:\n",
    "        return \"\"\n",
    "#print(datetime.timedelta(0, 8, 562000))\n",
    "#divmod(b.days * 86400 + b.seconds, 60)\n",
    "#datetime.timedelta(0, 8, 562000)\n",
    "\n",
    "def durationByHoursHopefully(eegReport): #last ditch effort to match any mention of days or hours in the report\n",
    "    hours = 0 \n",
    "    days = 0\n",
    "    i=0\n",
    "    for match in re.finditer(eegDurationPattern4, eegReport):\n",
    "        matchtxt = match.group(2)\n",
    "        i += 1\n",
    "        #print (\"hi\" + repr(i) + match.group(1) + matchtxt)\n",
    "        if \"hour\" in matchtxt:\n",
    "            hours =  int(float(match.group(1)))\n",
    "        elif \"day\" in matchtxt:\n",
    "            days = int(float(match.group(1)))\n",
    "            #print(\"################################\")\n",
    "    totalhours=  hours + 24*days\n",
    "    #print (totalhours)\n",
    "    return repr(totalhours) + ':00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentence_score_deprecated(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentence_score(sentences):    \n",
    "    scored_sentences = []\n",
    "    if not sentences:\n",
    "        return None\n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            tagged_expression = []\n",
    "            token_score = 0\n",
    "            for (word, wordstem, postag) in sentence:\n",
    "                if 'inc' in postag:\n",
    "                    token_score *= 2.0\n",
    "                elif 'dec' in postag:\n",
    "                    token_score /= 2.0\n",
    "                elif 'inv' in postag:\n",
    "                    token_score += -1             \n",
    "                else:\n",
    "                    token_score = 0\n",
    "                tagged = (word, wordstem, postag, token_score)\n",
    "                tagged_expression.append(tagged)\n",
    "            scored_sentences.append(tagged_expression)\n",
    "    return scored_sentences\n",
    "\n",
    "def sentence_simplify(sentences):    \n",
    "    scored_sentences = []\n",
    "    if not sentences:\n",
    "        return None\n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            tagged_expression = []\n",
    "            for (word, wordstem, postag) in sentence:\n",
    "                #print(postag)\n",
    "                if ('DT' in postag) or \\\n",
    "                    ('IN' in postag) or \\\n",
    "                    ('NN' in postag) or \\\n",
    "                    ('RB' in postag) or \\\n",
    "                    ('JJ' in postag) or \\\n",
    "                    ('NNS' in postag):\n",
    "                    tagged = (word, wordstem, postag)\n",
    "                    tagged_expression.append(tagged)\n",
    "                    #print('ok')\n",
    "            if tagged_expression:\n",
    "                scored_sentences.append(tagged_expression)\n",
    "    return scored_sentences\n",
    "\n",
    "\n",
    "def sentiment_score(review):\n",
    "    finalized_score = 0\n",
    "    if (review is None):\n",
    "        return 0\n",
    "    for sentence in review:\n",
    "        if sentence:\n",
    "            #pprint(sentence)\n",
    "            token_score = 0\n",
    "            for (word, wordstem, tags, score) in sentence:\n",
    "                token_score += sum([value_of(tag) for tag in tags])\n",
    "            #print(token_score)\n",
    "            for (word, wordstem, tags, score) in sentence:\n",
    "                if score != 0:\n",
    "                    token_score *= score \n",
    "                    #print(token_score)\n",
    "            finalized_score += token_score\n",
    "    return finalized_score\n",
    "\n",
    "        \n",
    "def sentimentAnalysis(text):\n",
    "\n",
    "    splitter = Splitter()\n",
    "    postagger = POSTagger()\n",
    "    dicttagger = DictionaryTagger([ 'dicts/positive.yml', 'dicts/negative.yml', \n",
    "                                    'dicts/inc.yml', 'dicts/dec.yml', 'dicts/inv.yml'])\n",
    "\n",
    "    if isinstance(text, basestring):\n",
    "        splitted_sentences = splitter.split(text)\n",
    "    else:\n",
    "        return 0\n",
    "    #pprint(text)\n",
    "    #pprint(splitted_sentences)\n",
    "\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint(pos_tagged_sentences)\n",
    "\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "\n",
    "    dict_tagged_sentences = sentence_simplify(dict_tagged_sentences)\n",
    "    #print(dict_tagged_sentences)\n",
    "    #() + 1\n",
    "    score_tagged_sentences = sentence_score(dict_tagged_sentences)\n",
    "    #print(score_tagged_sentences)\n",
    "    #print(\"analyzing sentiment...\")\n",
    "    score = sentiment_score(score_tagged_sentences)\n",
    "    #print(score)    \n",
    "    return score\n",
    "\n",
    "#sentimentAnalysis(\"left temporal partial-onset ictal activity abnormal\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(3, 2177)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "def loadMasterCSVFile():\n",
    "    with codecs.open(\"NKdatabaseDump.csv\", 'r', encoding='utf-8', errors='ignore') as cf:\n",
    "        i = 0\n",
    "        masterCSV = csv.reader(cf)\n",
    "        masterCSV_list = [row for row in masterCSV]\n",
    "        print(len(masterCSV_list))\n",
    "        return masterCSV_list\n",
    "\n",
    "\n",
    "def durationByDatesForCSV(t1, t2):\n",
    "    try:\n",
    "        dtformat = '%m/%d/%Y %I:%M:%S %p'\n",
    "    #try:\n",
    "    #print(t1)\n",
    "        t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "        t2 = datetime.datetime.strptime(t2, dtformat)\n",
    "    #print(t2)\n",
    "    #t1 = t1.date()\n",
    "    #t2 = t2.date()\n",
    "    except ValueError:\n",
    "        try:\n",
    "            dtformat = '%m/%d/%Y'\n",
    "            t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "            t2 = datetime.datetime.strptime(t2, dtformat)        \n",
    "        except:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    td = t2-t1\n",
    "    return td\n",
    "    #print(td)\n",
    "    return (repr(td.days*24) +\":\"+ repr(td.seconds//3600).zfill(2)  +\":\"+ repr((td.seconds//60)%60).zfill(2) )\n",
    "    #except:\n",
    "    #    return \"\"\n",
    "    \n",
    "# return a dateDelta for any given exam number by looking up all the rows with an exam number, \n",
    "# then looking for the minimal and maximal dates within those rows\n",
    "def masterCSV_daterange(examNO):\n",
    "    #print(len(masterCSV_list))\n",
    "    #columns=masterCSV_list[0]\n",
    "    #print(columns)\n",
    "    df = pd.DataFrame(masterCSV_list[1:],\n",
    "                    columns=masterCSV_list[0])\n",
    "    #print(df)\n",
    "    #HTML(df.to_html())\n",
    "    #display(df)\n",
    "    #df = df.drop_duplicates()\n",
    "    po = df['INF_ScheduleExam@ExamNo_STR'].str.contains(examNO)\n",
    "    #display(po)\n",
    "    #idx = df[po].index.tolist()\n",
    "    #r = df.loc(idx)\n",
    "    #display(r)\n",
    "    resultingDF = df.iloc[np.flatnonzero(po)]\n",
    "    #display(r)\n",
    "    dateList = list()\n",
    "    dateList.extend(resultingDF['INF_ScheduleExam@StartTime_DT'].tolist())\n",
    "    dateList.extend(resultingDF['INF_ScheduleExam@EndTime_DT'].tolist())\n",
    "    #print(examNO)\n",
    "    #display(dateList)\n",
    "    if (len(dateList) is 0):\n",
    "        return None\n",
    "    max_value = max(dateList)\n",
    "    min_value = min(dateList)\n",
    "    #print(max_value)\n",
    "    #print(min_value)\n",
    "    #duration = max_value - min_value\n",
    "    duration = durationByDatesForCSV(min_value, max_value)\n",
    "    return duration\n",
    "    print(duration)\n",
    "    #mask = np.column_stack([df['INF_ScheduleExam@ExamNo_STR'].str.contains(examNO)])\n",
    "    #display(mask)\n",
    "    #examNo_col = [item[''] for item in masterCSV_list]\n",
    "    #a = np.array(masterCSV_list)\n",
    "    #i,j = np.where(a==examNO)\n",
    "    #x = enumerate(masterCSV_list) \n",
    "    #print(x.value)\n",
    "    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(masterCSV_list)))\n",
    "    #result = [i for i,x in enumerate(masterCSV_list) if x.==examNO] # => [1, 3]\n",
    "    #print(i,j)\n",
    "\n",
    "masterCSV_list = loadMasterCSVFile()\n",
    "masterCSV_daterange(\"A17-293\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here i amNone\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'basestring' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4a96cb5f8cf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[1;31m#write sentiment score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentimentScore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentimentAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'impressionBody'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'sz'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'impressionType'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentimentScore'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-19c47a4e0077>\u001b[0m in \u001b[0;36msentimentAnalysis\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    186\u001b[0m                                     'dicts/inc.yml', 'dicts/dec.yml', 'dicts/inv.yml'])\n\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0msplitted_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'basestring' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "i=1              \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    #optional offset \n",
    "    #for line in reader:\n",
    "    #    i += 1\n",
    "    #    if (i>9425):\n",
    "    #        i=1\n",
    "    #        break\n",
    "\n",
    "    outfieldnames = reader.fieldnames \n",
    "    outfieldnames.append('notebody')     \n",
    "    outfieldnames.append('impressionBody')     \n",
    "    outfieldnames.append('impression')     \n",
    "    outfieldnames.append('notetype')     \n",
    "    outfieldnames.append('duration')     \n",
    "    outfieldnames.append('impressionType')     \n",
    "    outfieldnames.append('sentimentScore')     \n",
    "    outfieldnames.append('isSeizure')     \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        #write the notebody and impression\n",
    "        if m:\n",
    "            #print(m.start())\n",
    "            #print(line['note'][:m.start()])\n",
    "            #break\n",
    "            line['notebody'] = line['note'][:m.start()]\n",
    "            eeg_no = m.group('eegno')\n",
    "            findTrueImpression(eeg_no,line,i)\n",
    "            setImpressionBody(m,line,i)\n",
    "        else:\n",
    "            #try a looser find that may introduce more false information (more sensitive less specific pattern)\n",
    "            m = re_eegnoLoose.search(line['note'])\n",
    "            if m:\n",
    "                line['notebody'] = line['note'][:m.start()]\n",
    "                setImpressionBody(m,line,i)\n",
    "                eeg_no = m.group('eegno')\n",
    "                findTrueImpression(eeg_no,line,i)\n",
    "            elif \"preliminary\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            elif \"prelim\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            else:\n",
    "                print(\"###IMPRESSION BLOCK MISSING-\" + repr(i) + line['note'])\n",
    "                print()\n",
    "\n",
    "                  \n",
    "        # figure out the duration by cross-reference the duration of the study based on the examno\n",
    "        m = re_eegnoType.search(line['note'])\n",
    "        line['duration'] = None\n",
    "        if m:\n",
    "            #print(\"got csv table lookup\")\n",
    "            line['duration'] = masterCSV_daterange(m.group(0))\n",
    "            #print(line['duration'])\n",
    "        if line['duration'] is None:                \n",
    "            #write the duration by some other messier means\n",
    "            m = re_eegnoDuration.search(line['note'])\n",
    "            if m:            \n",
    "                match = m.group(1)\n",
    "                line['duration'] = match\n",
    "            else:\n",
    "                m = re_eegnoDuration2.search(line['note'])\n",
    "                if m:\n",
    "                    match = m.group(1)\n",
    "                    #print(match)\n",
    "                    if \"hour\" in match:\n",
    "                        line['duration'] =  m.group(2) + ':00:00'\n",
    "                    elif \"day\" in match:\n",
    "                        line['duration'] = repr(int(m.group(3))*24) + ':00:00'  \n",
    "                else:\n",
    "                    m = re_eegnoDuration3.search(line['note'])\n",
    "                    if m:\n",
    "                        line['duration'] = durationByDates(m.group(1), m.group(2))\n",
    "                    else:\n",
    "                        m = re_eegnoDuration4.search(line['note'])\n",
    "                        if m:\n",
    "                            line['duration'] = durationByHoursHopefully(line['note'])\n",
    "                        else:\n",
    "                            m = re_eegnoDuration5.search(line['note'])\n",
    "                            print(\"here i am\" + repr(m))\n",
    "                            if m:\n",
    "                                print(m.group(1))\n",
    "                                line['duration'] = durationByDates(m.group(1), m.group(2))\n",
    "                            \n",
    "        #print(line['duration'])\n",
    "        #() + 1                                \n",
    "                    \n",
    "                            \n",
    "\n",
    "        #write the notetype\n",
    "        m = re_eegnoType.search(line['note'])\n",
    "        if m:\n",
    "            match = m.group(0)\n",
    "            match = match.lower()\n",
    "            if (\"s\" in match):\n",
    "                line['notetype'] = \"spot\"\n",
    "            elif (\"v\" in match):\n",
    "                line['notetype'] = \"ceeg\"\n",
    "            elif (\"a\" in match):\n",
    "                line['notetype'] = \"ambu\"\n",
    "            elif (\"f\" in match):\n",
    "                line['notetype'] = \"spot inpt\"\n",
    "            elif (\"e\" in match):\n",
    "                line['notetype'] = \"ceeg\"\n",
    "            else:\n",
    "                line['notetype'] = \"unk\"\n",
    "                \n",
    "        elif line['duration'] is not None:\n",
    "            #determine the unknown report based off the duration only\n",
    "            dtformat = '%H:%M:%S'\n",
    "            try:\n",
    "                t = datetime.datetime.strptime(line['duration'], dtformat)\n",
    "                t = t.minute + t.hour*60\n",
    "                if t < 100 and t > 18:\n",
    "                    line['notetype'] = \"spot?\"\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #write the abnormality type\n",
    "        setImpressionType(line,i)\n",
    "            \n",
    "        #write sentiment score\n",
    "        line['sentimentScore'] = sentimentAnalysis(line['impressionBody'])\n",
    "\n",
    "        if ('sz' in line['impressionType']) and int(line['sentimentScore']) < 0:\n",
    "            line['isSeizure'] = 1\n",
    "        elif ('sz' in line['impressionType']) ^ int(line['sentimentScore']) < 0:\n",
    "            line['isSeizure'] = '?'\n",
    " \n",
    "            \n",
    "        #print(repr(i) + \": \"+ line['impressionType'])\n",
    "        \n",
    "        if (line['notetype'] == \"ambu\"):        \n",
    "            writer.writerow(line)    \n",
    "        #writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + line['note_Id'][:20] + repr(line['sentimentScore']), i )\n",
    "        #print(\"line %s\" + line['notebody'][:200], i )\n",
    "        #print()\n",
    "        #if i > 555:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "#close(file_name)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import csvkit as csv\n",
    "\n",
    "allReports = \"\"\n",
    "with open(\"SHC_ALL_NOTES_FILTERED.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        allReports += line['note'] + \" \\n\\n\"\n",
    "        # print(desc)\n",
    "        # now manipulate the note body\n",
    "        \n",
    "f = open(\"allReports.txt\", \"w\")\n",
    "f.write(allReports)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a histogram of the most commonly used words in all EEG reports\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = open(\"allReports.txt\", \"r\")\n",
    "allReports = f.read() \n",
    "f.close()\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "allwords_stemmed = tokenize_and_stem(allReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get rid of stop words\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_words = [word for word in allwords_stemmed if word not in stopwords]\n",
    "\n",
    "letter_counts = Counter(filtered_words)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df = df.sort_values(by=0, ascending=0)\n",
    "df1 = df[:20]\n",
    "plt.show(block=True)\n",
    "df1.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# most common bigrams\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "report = allReports.lower()\n",
    "reportNLTK = nltk.Text(nltk.tokenize.word_tokenize(report))\n",
    "reportNLTK.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate some eeg reports for me by sequencing bigrams\n",
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "#print(type(text))\n",
    "corpus = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"allReports.txt\")\n",
    "words=corpus.words()\n",
    "#get rid of stop words\n",
    "#filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "#words = filtered_words\n",
    "\n",
    "#print(type(words))\n",
    "bigrams = nltk.bigrams(words)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "generate_model(cfd, 'seizure')\n",
    "#filtered_words = [word for word in word_list if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_model(cfd, 'epileptic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'slow')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'discharge')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'periodic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'impression')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'background')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'infantile')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'neonatal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'seizure')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'seizures')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'This')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'electrographic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'indicates')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'represent')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'generalized')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'slowing')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'focal')\n",
    "print(\"\")\n",
    "generate_model(cfd, '3')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'hz')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'second')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'button')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'cry')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'clonic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'tonic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'partial')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'sleep')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'vertex')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'temporal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'frontal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'occipital')\n",
    "print(\"\")\n",
    "#generate_model(cfd, 'hypsarrythmia')\n",
    "#print(\"\")\n",
    "generate_model(cfd, 'alpha')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'beta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'delta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'theta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'sudden')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'waxing')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'waning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fraction of words in text that are not in the stopword list\n",
    "def content_fraction(text):\n",
    "     stopwords = nltk.corpus.stopwords.words('english')\n",
    "     content = [w for w in text if w.lower() not in stopwords]\n",
    "     return len(content) / len(text)\n",
    "\n",
    "content_fraction(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#tutorial example of classification\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#tutorial continued\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#tutorial\n",
    "#what does it containt \n",
    "movie_reviews.categories()\n",
    "movie_reviews.fileids('neg')\n",
    "movie_reviews.categories()\n",
    "i = 1\n",
    "for doc in documents:\n",
    "    print(str(i) + \": \"+ doc[1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
    "print(featuresets[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "exclude = set(string.punctuation)\n",
    "print(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the most common word associated with the interpretation result (abnormal versus normal)\n",
    "#should take 10 mins\n",
    "import random\n",
    "import string\n",
    "documents = list()\n",
    "notebody = list()\n",
    "result = list()\n",
    "exclude = set(string.punctuation)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "with open(\"SHC_ALL_NOTES_IMPRESS.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        notebody = line['note'].lower()\n",
    "        notebodyStemmed = tokenize_and_stem(notebody)        \n",
    "        filtered_words = [word for word in notebodyStemmed if word not in stopwords]\n",
    "\n",
    "        #notebody = ''.join(ch for ch in notebody if ch not in exclude)\n",
    "        #notebody = notebody.split() \n",
    "        #notebody = [word for word in notebody if word not in stopwords.words('english')]\n",
    "        category = line['impression']\n",
    "        if (category == 'abnormal' or category == 'normal'):\n",
    "            documents.append((list(filtered_words), category))\n",
    "\n",
    "#documents[1]\n",
    "        # print(desc)\n",
    "        # now manipulate the note body\n",
    "\n",
    "random.shuffle(documents)\n",
    "#documents[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "i = 1\n",
    "for doc in documents:\n",
    "    print(doc[0])\n",
    "    i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constructing a list of the 2000 most frequent words in the overall corpus\n",
    "import nltk\n",
    "words = list()\n",
    "for doc in documents:\n",
    "    words += doc[0] \n",
    "    \n",
    "#print(words)    \n",
    "all_words = nltk.FreqDist(w for w in words)\n",
    "word_features = list(all_words)[:4000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "for (d,c) in documents:\n",
    "    print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ccount = int (len(featuresets) / 2) - 1\n",
    "#take half of the total as the training set and the other half as a test set\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[ccount:], featuresets[:ccount]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (d) in documents:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# in order to load a corpus, reports must be in separate files // don't need to run this again\n",
    "\n",
    "capture_eegno = r'(?:DATE OF SERVICE:|STUDY DATE:|DATE EEG:|Date:|Service Date:|Study date|Study dates|DATE OF EEG:|start|T:|test dates:|Date of study:|Exam date|exam date:)\\s*(?P<eegno>[\\d/-]+)\\s*'\n",
    "capture_eegno1 = r'(?:Date:|Study date |Study date:)\\s*(?P<eegno>(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)\\s*[,\\s\\d/-]+)(?:[a-z]+)'\n",
    "capture_eegno2 = r'(?<!DOB:  )(?P<eegno>[\\d]+/[\\d]+/[\\d]+)' #DOB:  10/04/1993   \n",
    "eegDateRange = r'(?P<eegno>[\\d/]+-[\\d/]+)'\n",
    "eegDateStrict = r'(?P<eegno>[\\d]+/[\\d]+/[\\d]+)'\n",
    "#capture_eegno = r'(?P<eegno>[0-9]+)\\s*'\n",
    "re_eegno = re.compile(capture_eegno, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno1 = re.compile(capture_eegno1, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno2 = re.compile(capture_eegno2, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateRange = re.compile(eegDateRange, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateStrict = re.compile(eegDateStrict, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "\n",
    "out_file = open(\"lpch_eeg_reports_interp_date_impression.csv\",'w')\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    outfieldnames = reader.fieldnames\n",
    "    outfieldnames.append('date')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='***')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for line in reader:\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        print(str(i) + \": \", end='')\n",
    "        #print(str(i) + \": \" + str(type(m)), end='')\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            #print(eeg_no)\n",
    "        else:\n",
    "            m = re_eegno1.search(line['note'])\n",
    "            if m:\n",
    "                eeg_no = m.group('eegno')\n",
    "                #print(eeg_no)\n",
    "            else:\n",
    "                #print(\"!!!!!!!!!!!\")\n",
    "                for match in re.finditer(capture_eegno2,line['note']):\n",
    "                    eeg_no = match.group('eegno')\n",
    "                    \n",
    "                #m = re_eegno2.search(line['note'])\n",
    "                #if m:\n",
    "                #    eeg_no = m.group('eegno')\n",
    "                #print(eeg_no +\"???????\")\n",
    "\n",
    "        #if len(eeg_no) > 12:\n",
    "        #    m = eegDateRange.search(eeg_no)            \n",
    "        #    if m:\n",
    "        #        print(\"bad\")\n",
    "        #        m = eegDateStrict.search(eeg_no)\n",
    "        #        m = eegDateStrict.search(eeg_no)\n",
    "        #        if m:\n",
    "        #            eeg_no = m.group('eegno')\n",
    "        if ((len(eeg_no) < 5) | (len(eeg_no) > 12)):\n",
    "            #print(\"possibly bad: \" + eeg_no)\n",
    "            for match in re.finditer(capture_eegno2,line['note']):\n",
    "                eeg_no = match.group('eegno')\n",
    "        line['date'] = eeg_no \n",
    "        writer.writerow(line)\n",
    "        \n",
    "        #print(eeg_no)    \n",
    "        #i += 1\n",
    "        #if (i>600):\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: 47: 48: 49: 50: 51: 52: 53: 54: 55: 56: 57: 58: 59: 60: 61: 62: 63: 64: 65: 66: 67: 68: 69: 70: 71: 72: 73: 74: 75: 76: 77: 78: 79: 80: 81: 82: 83: 84: 85: 86: 87: 88: 89: 90: 91: 92: 93: 94: 95: 96: 97: 98: 99: 100: 101: 102: 103: 104: 105: 106: 107: 108: 109: 110: 111: 112: 113: 114: 115: 116: 117: 118: 119: 120: 121: 122: 123: 124: 125: 126: 127: 128: 129: 130: 131: 132: 133: 134: 135: 136: 137: 138: 139: 140: 141: 142: 143: 144: 145: 146: 147: 148: 149: 150: 151: 152: 153: 154: 155: 156: 157: 158: 159: 160: 161: 162: 163: 164: 165: 166: 167: 168: 169: 170: 171: 172: 173: 174: 175: 176: 177: 178: 179: 180: 181: 182: 183: 184: 185: 186: 187: 188: 189: 190: 191: 192: 193: 194: 195: 196: 197: 198: 199: 200: 201: 202: 203: 204: 205: 206: 207: 208: 209: 210: 211: 212: 213: 214: 215: 216: 217: 218: 219: 220: 221: 222: 223: 224: 225: 226: 227: 228: 229: 230: 231: 232: 233: 234: 235: 236: 237: 238: 239: 240: 241: 242: 243: 244: 245: 246: 247: 248: 249: 250: 251: 252: 253: 254: 255: 256: 257: 258: 259: 260: 261: 262: 263: 264: 265: 266: 267: 268: 269: 270: 271: 272: 273: 274: 275: 276: 277: 278: 279: 280: 281: 282: 283: 284: 285: 286: 287: 288: 289: 290: 291: 292: 293: 294: 295: 296: 297: 298: 299: 300: 301: 302: 303: 304: 305: 306: 307: 308: 309: 310: 311: 312: 313: 314: 315: 316: 317: 318: 319: 320: 321: 322: 323: 324: 325: 326: 327: 328: 329: 330: 331: 332: 333: 334: 335: 336: 337: 338: 339: 340: 341: 342: 343: 344: 345: 346: 347: 348: 349: 350: 351: 352: 353: 354: 355: 356: 357: 358: 359: 360: 361: 362: 363: 364: 365: 366: 367: 368: 369: 370: 371: 372: 373: 374: 375: 376: 377: 378: 379: 380: 381: 382: 383: 384: 385: 386: 387: 388: 389: 390: 391: 392: 393: 394: 395: 396: 397: 398: 399: 400: 401: 402: 403: 404: 405: 406: 407: 408: 409: 410: 411: 412: 413: 414: 415: 416: 417: 418: 419: 420: 421: 422: 423: 424: 425: 426: 427: 428: 429: 430: 431: 432: 433: 434: 435: 436: 437: 438: 439: 440: 441: 442: 443: 444: 445: 446: 447: 448: 449: 450: 451: 452: 453: 454: 455: 456: 457: 458: 459: 460: 461: 462: 463: 464: 465: 466: 467: 468: 469: 470: 471: 472: 473: 474: 475: 476: 477: 478: 479: 480: 481: 482: 483: 484: 485: 486: 487: 488: 489: 490: 491: 492: 493: 494: 495: 496: 497: 498: 499: 500: 501: 502: 503: 504: 505: 506: 507: 508: 509: 510: 511: 512: 513: 514: 515: 516: 517: 518: 519: 520: 521: 522: 523: 524: 525: 526: 527: 528: 529: 530: 531: 532: 533: 534: 535: 536: 537: 538: 539: 540: 541: 542: 543: 544: 545: 546: 547: 548: 549: 550: 551: 552: 553: 554: 555: 556: 557: 558: 559: 560: 561: 562: 563: 564: 565: 566: 567: 568: 569: 570: 571: 572: 573: 574: 575: 576: 577: 578: 579: 580: 581: 582: 583: 584: 585: 586: 587: 588: 589: 590: 591: 592: 593: 594: 595: 596: 597: 598: 599: 600: 601: 602: 603: 604: 605: 606: 607: 608: 609: 610: 611: 612: 613: 614: 615: 616: 617: 618: 619: 620: 621: 622: 623: 624: 625: 626: 627: 628: 629: 630: 631: 632: 633: 634: 635: 636: 637: 638: 639: 640: 641: 642: 643: 644: 645: 646: 647: 648: 649: 650: 651: 652: 653: 654: 655: 656: 657: 658: 659: 660: 661: 662: 663: 664: 665: 666: 667: 668: 669: 670: 671: 672: 673: 674: 675: 676: 677: 678: 679: 680: 681: 682: 683: 684: 685: 686: 687: 688: 689: 690: 691: 692: 693: 694: 695: 696: 697: 698: 699: 700: 701: 702: 703: 704: 705: 706: 707: 708: 709: 710: 711: 712: 713: 714: 715: 716: 717: 718: 719: 720: 721: 722: 723: 724: 725: 726: 727: 728: 729: 730: 731: 732: 733: 734: 735: 736: 737: 738: 739: 740: 741: 742: 743: 744: 745: 746: 747: 748: 749: 750: 751: 752: 753: 754: 755: 756: 757: 758: 759: 760: 761: 762: 763: 764: 765: 766: 767: 768: 769: 770: 771: 772: 773: 774: 775: 776: 777: 778: 779: 780: 781: 782: 783: 784: 785: 786: 787: 788: 789: 790: 791: 792: 793: 794: 795: 796: 797: 798: 799: 800: 801: 802: 803: 804: 805: 806: 807: 808: 809: 810: 811: 812: 813: 814: 815: 816: 817: 818: 819: 820: 821: 822: 823: 824: 825: 826: 827: 828: 829: 830: 831: 832: 833: 834: 835: 836: 837: 838: 839: 840: 841: 842: 843: 844: 845: 846: 847: 848: 849: 850: 851: 852: 853: 854: 855: 856: 857: 858: 859: 860: 861: 862: 863: 864: 865: 866: 867: 868: 869: 870: 871: 872: 873: 874: 875: 876: 877: 878: 879: 880: 881: 882: 883: 884: 885: 886: 887: 888: 889: 890: 891: 892: 893: 894: 895: 896: 897: 898: 899: 900: 901: 902: 903: 904: 905: 906: 907: 908: 909: 910: 911: 912: 913: 914: 915: 916: 917: 918: 919: 920: 921: 922: 923: 924: 925: 926: 927: 928: 929: 930: 931: 932: 933: 934: 935: 936: 937: 938: 939: 940: 941: 942: 943: 944: 945: 946: 947: 948: 949: 950: 951: 952: 953: 954: 955: 956: 957: 958: 959: 960: 961: 962: 963: 964: 965: 966: 967: 968: 969: 970: 971: 972: 973: 974: 975: 976: 977: 978: 979: 980: 981: 982: 983: 984: 985: 986: 987: 988: 989: 990: 991: 992: 993: 994: 995: 996: 997: 998: 999: 1000: 1001: 1002: 1003: 1004: 1005: 1006: 1007: 1008: 1009: 1010: 1011: 1012: 1013: 1014: 1015: 1016: 1017: 1018: 1019: 1020: 1021: 1022: 1023: 1024: 1025: 1026: 1027: 1028: 1029: 1030: 1031: 1032: 1033: 1034: 1035: 1036: 1037: 1038: 1039: 1040: 1041: 1042: 1043: 1044: 1045: 1046: 1047: 1048: 1049: 1050: 1051: 1052: 1053: 1054: 1055: 1056: 1057: 1058: 1059: 1060: 1061: 1062: 1063: 1064: 1065: 1066: 1067: 1068: 1069: 1070: 1071: 1072: 1073: 1074: 1075: 1076: 1077: 1078: 1079: 1080: 1081: 1082: 1083: 1084: 1085: 1086: 1087: 1088: 1089: 1090: 1091: 1092: 1093: 1094: 1095: 1096: 1097: 1098: 1099: 1100: 1101: 1102: 1103: 1104: 1105: 1106: 1107: 1108: 1109: 1110: 1111: 1112: 1113: 1114: 1115: 1116: 1117: 1118: 1119: 1120: 1121: 1122: 1123: 1124: 1125: 1126: 1127: 1128: 1129: 1130: 1131: 1132: 1133: 1134: 1135: 1136: 1137: 1138: 1139: 1140: 1141: 1142: 1143: 1144: 1145: 1146: 1147: 1148: 1149: 1150: 1151: 1152: 1153: 1154: 1155: 1156: 1157: 1158: 1159: 1160: 1161: 1162: 1163: 1164: 1165: 1166: 1167: 1168: 1169: 1170: 1171: 1172: 1173: 1174: 1175: 1176: 1177: 1178: 1179: 1180: 1181: 1182: 1183: 1184: 1185: 1186: 1187: 1188: 1189: 1190: 1191: 1192: 1193: 1194: 1195: 1196: 1197: 1198: 1199: 1200: 1201: 1202: 1203: 1204: 1205: 1206: 1207: 1208: 1209: 1210: 1211: 1212: 1213: 1214: 1215: 1216: 1217: 1218: 1219: 1220: 1221: 1222: 1223: 1224: 1225: 1226: 1227: 1228: 1229: 1230: 1231: 1232: 1233: 1234: 1235: 1236: 1237: 1238: 1239: 1240: 1241: 1242: 1243: 1244: 1245: 1246: 1247: 1248: 1249: 1250: 1251: 1252: 1253: 1254: 1255: 1256: 1257: 1258: 1259: 1260: 1261: 1262: 1263: 1264: 1265: 1266: 1267: 1268: 1269: 1270: 1271: 1272: 1273: 1274: 1275: 1276: 1277: 1278: 1279: 1280: 1281: 1282: 1283: 1284: 1285: 1286: 1287: 1288: 1289: 1290: 1291: 1292: 1293: 1294: 1295: 1296: 1297: 1298: 1299: 1300: 1301: 1302: 1303: 1304: 1305: 1306: 1307: 1308: 1309: 1310: 1311: 1312: 1313: 1314: 1315: 1316: 1317: 1318: 1319: 1320: 1321: 1322: 1323: 1324: 1325: 1326: 1327: 1328: 1329: 1330: 1331: 1332: 1333: 1334: 1335: 1336: 1337: 1338: 1339: 1340: 1341: 1342: 1343: 1344: 1345: 1346: 1347: 1348: 1349: 1350: 1351: 1352: 1353: 1354: 1355: 1356: 1357: 1358: 1359: 1360: 1361: 1362: 1363: 1364: 1365: 1366: 1367: 1368: 1369: 1370: 1371: 1372: 1373: 1374: 1375: 1376: 1377: 1378: 1379: 1380: 1381: 1382: 1383: 1384: 1385: 1386: 1387: 1388: 1389: 1390: 1391: 1392: 1393: 1394: 1395: 1396: 1397: 1398: 1399: 1400: 1401: 1402: 1403: 1404: 1405: 1406: 1407: 1408: 1409: 1410: 1411: 1412: 1413: 1414: 1415: 1416: 1417: 1418: 1419: 1420: 1421: 1422: 1423: 1424: 1425: 1426: 1427: 1428: 1429: 1430: 1431: 1432: 1433: 1434: 1435: 1436: 1437: 1438: 1439: 1440: 1441: 1442: 1443: 1444: 1445: 1446: 1447: 1448: 1449: 1450: 1451: 1452: 1453: 1454: 1455: 1456: 1457: 1458: 1459: 1460: 1461: 1462: 1463: 1464: 1465: 1466: 1467: 1468: 1469: 1470: 1471: 1472: 1473: 1474: 1475: 1476: 1477: 1478: 1479: 1480: 1481: 1482: 1483: 1484: 1485: 1486: 1487: 1488: 1489: 1490: 1491: 1492: 1493: 1494: 1495: 1496: 1497: 1498: 1499: 1500: 1501: 1502: 1503: 1504: 1505: 1506: 1507: 1508: 1509: 1510: 1511: 1512: 1513: 1514: 1515: 1516: 1517: 1518: 1519: 1520: 1521: 1522: 1523: 1524: 1525: 1526: 1527: 1528: 1529: 1530: 1531: 1532: 1533: 1534: 1535: 1536: 1537: 1538: 1539: 1540: 1541: 1542: 1543: 1544: 1545: 1546: 1547: 1548: 1549: 1550: 1551: 1552: 1553: 1554: 1555: 1556: 1557: 1558: 1559: 1560: 1561: 1562: 1563: 1564: 1565: 1566: 1567: 1568: 1569: 1570: 1571: 1572: 1573: 1574: 1575: 1576: 1577: 1578: 1579: 1580: 1581: 1582: 1583: 1584: 1585: 1586: 1587: 1588: 1589: 1590: 1591: 1592: 1593: 1594: 1595: 1596: 1597: 1598: 1599: 1600: 1601: 1602: 1603: 1604: 1605: 1606: 1607: 1608: 1609: 1610: 1611: 1612: 1613: 1614: 1615: 1616: 1617: 1618: 1619: 1620: 1621: 1622: 1623: 1624: 1625: 1626: 1627: 1628: 1629: 1630: 1631: 1632: 1633: 1634: 1635: 1636: 1637: 1638: 1639: 1640: 1641: 1642: 1643: 1644: 1645: 1646: 1647: 1648: 1649: 1650: 1651: 1652: 1653: 1654: 1655: 1656: 1657: 1658: 1659: 1660: 1661: 1662: 1663: 1664: 1665: 1666: 1667: 1668: 1669: 1670: 1671: 1672: 1673: 1674: 1675: 1676: 1677: 1678: 1679: 1680: 1681: 1682: 1683: 1684: 1685: 1686: 1687: 1688: 1689: 1690: 1691: 1692: 1693: 1694: 1695: 1696: 1697: 1698: 1699: 1700: 1701: 1702: 1703: 1704: 1705: 1706: 1707: 1708: 1709: 1710: 1711: 1712: 1713: 1714: 1715: 1716: 1717: 1718: 1719: 1720: 1721: 1722: 1723: 1724: 1725: 1726: 1727: 1728: 1729: 1730: 1731: 1732: 1733: 1734: 1735: 1736: 1737: 1738: 1739: 1740: 1741: 1742: 1743: 1744: 1745: 1746: 1747: 1748: 1749: 1750: 1751: 1752: 1753: 1754: 1755: 1756: 1757: 1758: 1759: 1760: 1761: 1762: 1763: 1764: 1765: 1766: 1767: 1768: 1769: 1770: 1771: 1772: 1773: 1774: 1775: 1776: 1777: 1778: 1779: 1780: 1781: 1782: 1783: 1784: 1785: 1786: 1787: 1788: 1789: 1790: 1791: 1792: 1793: 1794: 1795: 1796: 1797: 1798: 1799: 1800: 1801: 1802: 1803: 1804: 1805: 1806: 1807: 1808: 1809: 1810: 1811: 1812: 1813: 1814: 1815: 1816: 1817: 1818: 1819: 1820: 1821: 1822: 1823: 1824: 1825: 1826: 1827: 1828: 1829: 1830: 1831: 1832: 1833: 1834: 1835: 1836: 1837: 1838: 1839: 1840: 1841: 1842: 1843: 1844: 1845: 1846: 1847: 1848: 1849: 1850: 1851: 1852: 1853: 1854: 1855: 1856: 1857: 1858: 1859: 1860: 1861: 1862: 1863: 1864: 1865: 1866: 1867: 1868: 1869: 1870: 1871: 1872: 1873: 1874: 1875: 1876: 1877: 1878: 1879: 1880: 1881: 1882: 1883: 1884: 1885: 1886: 1887: 1888: 1889: 1890: 1891: 1892: 1893: 1894: 1895: 1896: 1897: 1898: 1899: 1900: 1901: 1902: 1903: 1904: 1905: 1906: 1907: 1908: 1909: 1910: 1911: 1912: 1913: 1914: 1915: 1916: 1917: 1918: 1919: 1920: 1921: 1922: 1923: 1924: 1925: 1926: 1927: 1928: 1929: 1930: 1931: 1932: 1933: 1934: 1935: 1936: 1937: 1938: 1939: 1940: 1941: 1942: 1943: 1944: 1945: 1946: 1947: 1948: 1949: 1950: 1951: 1952: 1953: 1954: 1955: 1956: 1957: 1958: 1959: 1960: 1961: 1962: 1963: 1964: 1965: 1966: 1967: 1968: 1969: 1970: 1971: 1972: 1973: 1974: 1975: 1976: 1977: 1978: 1979: 1980: 1981: 1982: 1983: 1984: 1985: 1986: 1987: 1988: 1989: 1990: 1991: 1992: 1993: 1994: 1995: 1996: 1997: 1998: 1999: 2000: 2001: 2002: 2003: 2004: 2005: 2006: 2007: 2008: 2009: 2010: 2011: 2012: 2013: 2014: 2015: 2016: 2017: 2018: 2019: 2020: 2021: 2022: 2023: 2024: 2025: 2026: 2027: 2028: 2029: 2030: 2031: 2032: 2033: 2034: 2035: 2036: 2037: 2038: 2039: 2040: 2041: 2042: 2043: 2044: 2045: 2046: 2047: 2048: 2049: 2050: 2051: 2052: 2053: 2054: 2055: 2056: 2057: 2058: 2059: 2060: 2061: 2062: 2063: 2064: 2065: 2066: 2067: 2068: 2069: 2070: 2071: 2072: 2073: 2074: 2075: 2076: 2077: 2078: 2079: 2080: 2081: 2082: 2083: 2084: 2085: 2086: 2087: 2088: 2089: 2090: 2091: 2092: 2093: 2094: 2095: 2096: 2097: 2098: 2099: 2100: 2101: 2102: 2103: 2104: 2105: 2106: 2107: 2108: 2109: 2110: 2111: 2112: 2113: 2114: 2115: 2116: 2117: 2118: 2119: 2120: 2121: 2122: 2123: 2124: 2125: 2126: 2127: 2128: 2129: 2130: 2131: 2132: 2133: 2134: 2135: 2136: 2137: 2138: 2139: 2140: 2141: 2142: 2143: 2144: 2145: 2146: 2147: 2148: 2149: 2150: 2151: 2152: 2153: 2154: 2155: 2156: 2157: 2158: 2159: 2160: 2161: 2162: 2163: 2164: 2165: 2166: 2167: 2168: 2169: 2170: 2171: 2172: 2173: 2174: 2175: 2176: 2177: 2178: 2179: 2180: 2181: 2182: 2183: 2184: 2185: 2186: 2187: 2188: 2189: 2190: 2191: 2192: 2193: 2194: 2195: 2196: 2197: 2198: 2199: 2200: 2201: 2202: 2203: 2204: 2205: 2206: 2207: 2208: 2209: 2210: 2211: 2212: 2213: 2214: 2215: 2216: 2217: 2218: 2219: 2220: 2221: 2222: 2223: 2224: 2225: 2226: 2227: 2228: 2229: 2230: 2231: 2232: 2233: 2234: 2235: 2236: 2237: 2238: 2239: 2240: 2241: 2242: 2243: 2244: 2245: 2246: 2247: 2248: 2249: 2250: 2251: 2252: 2253: 2254: 2255: 2256: 2257: 2258: 2259: 2260: 2261: 2262: 2263: 2264: 2265: 2266: 2267: 2268: 2269: 2270: 2271: 2272: 2273: 2274: 2275: 2276: 2277: 2278: 2279: 2280: 2281: 2282: 2283: 2284: 2285: 2286: 2287: 2288: 2289: 2290: 2291: 2292: 2293: 2294: 2295: 2296: 2297: 2298: 2299: 2300: 2301: 2302: 2303: 2304: 2305: 2306: 2307: 2308: 2309: 2310: 2311: 2312: 2313: 2314: 2315: 2316: 2317: 2318: 2319: 2320: 2321: 2322: 2323: 2324: 2325: 2326: 2327: 2328: 2329: 2330: 2331: 2332: 2333: 2334: 2335: 2336: 2337: 2338: 2339: 2340: 2341: 2342: 2343: 2344: 2345: 2346: 2347: 2348: 2349: 2350: 2351: 2352: 2353: 2354: 2355: 2356: 2357: 2358: 2359: 2360: 2361: 2362: 2363: 2364: 2365: 2366: 2367: 2368: 2369: 2370: 2371: 2372: 2373: 2374: 2375: 2376: 2377: 2378: 2379: 2380: 2381: 2382: 2383: 2384: 2385: 2386: 2387: 2388: 2389: 2390: 2391: 2392: 2393: 2394: 2395: 2396: 2397: 2398: 2399: 2400: 2401: 2402: 2403: 2404: 2405: 2406: 2407: 2408: 2409: 2410: 2411: 2412: 2413: 2414: 2415: 2416: 2417: 2418: 2419: 2420: 2421: 2422: 2423: 2424: 2425: 2426: 2427: 2428: 2429: 2430: 2431: 2432: 2433: 2434: 2435: 2436: 2437: 2438: 2439: 2440: 2441: 2442: 2443: 2444: 2445: 2446: 2447: 2448: 2449: 2450: 2451: 2452: 2453: 2454: 2455: 2456: 2457: 2458: 2459: 2460: 2461: 2462: 2463: 2464: 2465: 2466: 2467: 2468: 2469: 2470: 2471: 2472: 2473: 2474: 2475: 2476: 2477: 2478: 2479: 2480: 2481: 2482: 2483: 2484: 2485: 2486: 2487: 2488: 2489: 2490: 2491: 2492: 2493: 2494: 2495: 2496: 2497: 2498: 2499: 2500: 2501: 2502: 2503: 2504: 2505: 2506: 2507: 2508: 2509: 2510: 2511: 2512: 2513: 2514: 2515: 2516: 2517: 2518: 2519: 2520: 2521: 2522: 2523: 2524: 2525: 2526: 2527: 2528: 2529: 2530: 2531: 2532: 2533: 2534: 2535: 2536: 2537: 2538: 2539: 2540: 2541: 2542: 2543: 2544: 2545: 2546: 2547: 2548: 2549: 2550: 2551: 2552: 2553: 2554: 2555: 2556: 2557: 2558: 2559: 2560: 2561: 2562: 2563: 2564: 2565: 2566: 2567: 2568: 2569: 2570: 2571: 2572: 2573: 2574: 2575: 2576: 2577: 2578: 2579: 2580: 2581: 2582: 2583: 2584: 2585: 2586: 2587: 2588: 2589: 2590: 2591: 2592: 2593: 2594: 2595: 2596: 2597: 2598: 2599: 2600: 2601: 2602: 2603: 2604: 2605: 2606: 2607: 2608: 2609: 2610: 2611: 2612: 2613: 2614: 2615: 2616: 2617: 2618: 2619: 2620: 2621: 2622: 2623: 2624: 2625: 2626: 2627: 2628: 2629: 2630: 2631: 2632: 2633: 2634: 2635: 2636: 2637: 2638: 2639: 2640: 2641: 2642: 2643: 2644: 2645: 2646: 2647: 2648: 2649: 2650: 2651: 2652: 2653: 2654: 2655: 2656: 2657: 2658: 2659: 2660: 2661: 2662: 2663: 2664: 2665: 2666: 2667: 2668: 2669: 2670: 2671: 2672: 2673: 2674: 2675: 2676: 2677: 2678: 2679: 2680: 2681: 2682: 2683: 2684: 2685: 2686: 2687: 2688: 2689: 2690: 2691: 2692: 2693: 2694: 2695: 2696: 2697: 2698: 2699: 2700: 2701: 2702: 2703: 2704: 2705: 2706: 2707: 2708: 2709: 2710: 2711: 2712: 2713: 2714: 2715: 2716: 2717: 2718: 2719: 2720: 2721: 2722: 2723: 2724: 2725: 2726: 2727: 2728: 2729: 2730: 2731: 2732: 2733: 2734: 2735: 2736: 2737: 2738: 2739: 2740: 2741: 2742: 2743: 2744: 2745: 2746: 2747: 2748: 2749: 2750: 2751: 2752: 2753: 2754: 2755: 2756: 2757: 2758: 2759: 2760: 2761: 2762: 2763: 2764: 2765: 2766: 2767: 2768: 2769: 2770: 2771: 2772: 2773: 2774: 2775: 2776: 2777: 2778: 2779: 2780: 2781: 2782: 2783: 2784: 2785: 2786: 2787: 2788: 2789: 2790: 2791: 2792: 2793: 2794: 2795: 2796: 2797: 2798: 2799: 2800: 2801: 2802: 2803: 2804: 2805: 2806: 2807: 2808: 2809: 2810: 2811: 2812: 2813: 2814: 2815: 2816: 2817: 2818: 2819: 2820: 2821: 2822: 2823: 2824: 2825: 2826: 2827: 2828: 2829: 2830: 2831: 2832: 2833: 2834: 2835: 2836: 2837: 2838: 2839: 2840: 2841: 2842: 2843: 2844: 2845: 2846: 2847: 2848: 2849: 2850: 2851: 2852: 2853: 2854: 2855: 2856: 2857: 2858: 2859: 2860: 2861: 2862: 2863: 2864: 2865: 2866: 2867: 2868: 2869: 2870: 2871: 2872: 2873: 2874: 2875: 2876: 2877: 2878: 2879: 2880: 2881: 2882: 2883: 2884: 2885: 2886: 2887: 2888: 2889: 2890: 2891: 2892: 2893: 2894: 2895: 2896: 2897: 2898: 2899: 2900: 2901: 2902: 2903: 2904: 2905: 2906: 2907: 2908: 2909: 2910: 2911: 2912: 2913: 2914: 2915: 2916: 2917: 2918: 2919: 2920: 2921: 2922: 2923: 2924: 2925: 2926: 2927: 2928: 2929: 2930: 2931: 2932: 2933: 2934: 2935: 2936: 2937: 2938: 2939: 2940: 2941: 2942: 2943: 2944: 2945: 2946: 2947: 2948: 2949: 2950: 2951: 2952: 2953: 2954: 2955: 2956: 2957: 2958: 2959: 2960: 2961: 2962: 2963: 2964: 2965: 2966: 2967: 2968: 2969: 2970: 2971: 2972: 2973: 2974: 2975: 2976: 2977: 2978: 2979: 2980: 2981: 2982: 2983: 2984: 2985: 2986: 2987: 2988: 2989: 2990: 2991: 2992: 2993: 2994: 2995: 2996: 2997: 2998: 2999: 3000: 3001: 3002: 3003: 3004: 3005: 3006: 3007: 3008: 3009: 3010: 3011: 3012: 3013: 3014: 3015: 3016: 3017: 3018: 3019: 3020: 3021: 3022: 3023: 3024: 3025: 3026: 3027: 3028: 3029: 3030: 3031: 3032: 3033: 3034: 3035: 3036: 3037: 3038: 3039: 3040: 3041: 3042: 3043: 3044: 3045: 3046: 3047: 3048: 3049: 3050: 3051: 3052: 3053: 3054: 3055: 3056: 3057: 3058: 3059: 3060: 3061: 3062: 3063: 3064: 3065: 3066: 3067: 3068: 3069: 3070: 3071: 3072: 3073: 3074: 3075: 3076: 3077: 3078: 3079: 3080: 3081: 3082: 3083: 3084: 3085: 3086: 3087: 3088: 3089: 3090: 3091: 3092: 3093: 3094: 3095: 3096: 3097: 3098: 3099: 3100: 3101: 3102: 3103: 3104: 3105: 3106: 3107: 3108: 3109: 3110: 3111: 3112: 3113: 3114: 3115: 3116: 3117: 3118: 3119: 3120: 3121: 3122: 3123: 3124: 3125: 3126: 3127: 3128: 3129: 3130: 3131: 3132: 3133: 3134: 3135: 3136: 3137: 3138: 3139: 3140: 3141: 3142: 3143: 3144: 3145: 3146: 3147: 3148: 3149: 3150: 3151: 3152: 3153: 3154: 3155: 3156: 3157: 3158: 3159: 3160: 3161: 3162: 3163: 3164: 3165: 3166: 3167: 3168: 3169: 3170: 3171: 3172: 3173: 3174: 3175: 3176: 3177: 3178: 3179: 3180: 3181: 3182: 3183: 3184: 3185: 3186: 3187: 3188: 3189: 3190: 3191: 3192: 3193: 3194: 3195: 3196: 3197: 3198: 3199: 3200: 3201: 3202: 3203: 3204: 3205: 3206: 3207: 3208: 3209: 3210: 3211: 3212: 3213: 3214: 3215: 3216: 3217: 3218: 3219: 3220: 3221: 3222: 3223: 3224: 3225: 3226: 3227: 3228: 3229: 3230: 3231: 3232: 3233: 3234: 3235: 3236: 3237: 3238: 3239: 3240: 3241: 3242: 3243: 3244: 3245: 3246: 3247: 3248: 3249: 3250: 3251: 3252: 3253: 3254: 3255: 3256: 3257: 3258: 3259: 3260: 3261: 3262: 3263: 3264: 3265: 3266: 3267: 3268: 3269: 3270: 3271: 3272: 3273: 3274: 3275: 3276: 3277: 3278: 3279: 3280: 3281: 3282: 3283: 3284: 3285: 3286: 3287: 3288: 3289: 3290: 3291: 3292: 3293: 3294: 3295: 3296: 3297: 3298: 3299: 3300: 3301: 3302: 3303: 3304: 3305: 3306: 3307: 3308: 3309: 3310: 3311: 3312: 3313: 3314: 3315: 3316: 3317: 3318: 3319: 3320: 3321: 3322: 3323: 3324: 3325: 3326: 3327: 3328: 3329: 3330: 3331: 3332: 3333: 3334: 3335: 3336: 3337: 3338: 3339: 3340: 3341: 3342: 3343: 3344: 3345: 3346: 3347: 3348: 3349: 3350: 3351: 3352: 3353: 3354: 3355: 3356: 3357: 3358: 3359: 3360: 3361: 3362: 3363: 3364: 3365: 3366: 3367: 3368: 3369: 3370: 3371: 3372: 3373: 3374: 3375: 3376: 3377: 3378: 3379: 3380: 3381: 3382: 3383: 3384: 3385: 3386: 3387: 3388: 3389: 3390: 3391: 3392: 3393: 3394: 3395: 3396: 3397: 3398: 3399: 3400: 3401: 3402: 3403: 3404: 3405: 3406: 3407: 3408: 3409: 3410: 3411: 3412: 3413: 3414: 3415: 3416: 3417: 3418: 3419: 3420: 3421: 3422: 3423: 3424: 3425: 3426: 3427: 3428: 3429: 3430: 3431: 3432: 3433: 3434: 3435: 3436: 3437: 3438: 3439: 3440: 3441: 3442: 3443: 3444: 3445: 3446: 3447: 3448: 3449: 3450: 3451: 3452: 3453: 3454: 3455: 3456: 3457: 3458: 3459: 3460: 3461: 3462: 3463: 3464: 3465: 3466: 3467: 3468: 3469: 3470: 3471: 3472: 3473: 3474: 3475: 3476: 3477: 3478: 3479: 3480: 3481: 3482: 3483: 3484: 3485: 3486: 3487: 3488: 3489: 3490: 3491: 3492: 3493: 3494: 3495: 3496: 3497: 3498: 3499: 3500: 3501: 3502: 3503: 3504: 3505: 3506: 3507: 3508: 3509: 3510: 3511: 3512: 3513: 3514: 3515: 3516: 3517: 3518: 3519: 3520: 3521: 3522: 3523: 3524: 3525: 3526: 3527: 3528: 3529: 3530: 3531: 3532: 3533: 3534: 3535: 3536: 3537: 3538: 3539: 3540: 3541: 3542: 3543: 3544: 3545: 3546: 3547: 3548: 3549: 3550: 3551: 3552: 3553: 3554: 3555: 3556: 3557: 3558: 3559: 3560: 3561: 3562: 3563: 3564: 3565: 3566: 3567: 3568: 3569: 3570: 3571: 3572: 3573: 3574: 3575: 3576: 3577: 3578: 3579: 3580: 3581: 3582: 3583: 3584: 3585: 3586: 3587: 3588: 3589: 3590: 3591: 3592: 3593: 3594: 3595: 3596: 3597: 3598: 3599: 3600: 3601: 3602: 3603: 3604: 3605: 3606: 3607: 3608: 3609: 3610: 3611: 3612: 3613: 3614: 3615: 3616: 3617: 3618: 3619: 3620: 3621: 3622: 3623: 3624: 3625: 3626: 3627: 3628: 3629: 3630: 3631: 3632: 3633: 3634: 3635: 3636: 3637: 3638: 3639: 3640: 3641: 3642: 3643: 3644: 3645: 3646: 3647: 3648: 3649: 3650: 3651: 3652: 3653: 3654: 3655: 3656: 3657: 3658: 3659: 3660: 3661: 3662: 3663: 3664: 3665: 3666: 3667: 3668: 3669: 3670: 3671: 3672: 3673: 3674: 3675: 3676: 3677: 3678: 3679: 3680: 3681: 3682: 3683: 3684: 3685: 3686: 3687: 3688: 3689: 3690: 3691: 3692: 3693: 3694: 3695: 3696: 3697: 3698: 3699: 3700: 3701: 3702: 3703: 3704: 3705: 3706: 3707: 3708: 3709: 3710: 3711: 3712: 3713: 3714: 3715: 3716: 3717: 3718: 3719: 3720: 3721: 3722: 3723: 3724: 3725: 3726: 3727: 3728: 3729: 3730: 3731: 3732: 3733: 3734: 3735: 3736: 3737: 3738: 3739: 3740: 3741: 3742: 3743: 3744: 3745: 3746: 3747: 3748: 3749: 3750: 3751: 3752: 3753: 3754: 3755: 3756: 3757: 3758: 3759: 3760: 3761: 3762: 3763: 3764: 3765: 3766: 3767: 3768: 3769: 3770: 3771: 3772: 3773: 3774: 3775: 3776: 3777: 3778: 3779: 3780: 3781: 3782: 3783: 3784: 3785: 3786: 3787: 3788: 3789: 3790: 3791: 3792: 3793: 3794: 3795: 3796: 3797: 3798: 3799: 3800: 3801: 3802: 3803: 3804: 3805: 3806: 3807: 3808: 3809: 3810: 3811: 3812: 3813: 3814: 3815: 3816: 3817: 3818: 3819: 3820: 3821: 3822: 3823: 3824: 3825: 3826: 3827: 3828: 3829: 3830: 3831: 3832: 3833: 3834: 3835: 3836: 3837: 3838: 3839: 3840: 3841: 3842: 3843: 3844: 3845: 3846: 3847: 3848: 3849: 3850: 3851: 3852: 3853: 3854: 3855: 3856: 3857: 3858: 3859: 3860: 3861: 3862: 3863: 3864: 3865: 3866: 3867: 3868: 3869: 3870: 3871: 3872: 3873: 3874: 3875: 3876: 3877: 3878: 3879: 3880: 3881: 3882: 3883: 3884: 3885: 3886: 3887: 3888: 3889: 3890: 3891: 3892: 3893: 3894: 3895: 3896: 3897: 3898: 3899: 3900: 3901: 3902: 3903: 3904: 3905: 3906: 3907: 3908: 3909: 3910: 3911: 3912: 3913: 3914: 3915: 3916: 3917: 3918: 3919: 3920: 3921: 3922: 3923: 3924: 3925: 3926: 3927: 3928: 3929: 3930: 3931: 3932: 3933: 3934: 3935: 3936: 3937: 3938: 3939: 3940: 3941: 3942: 3943: 3944: 3945: 3946: 3947: 3948: 3949: 3950: 3951: 3952: 3953: 3954: 3955: 3956: 3957: 3958: 3959: 3960: 3961: 3962: 3963: 3964: 3965: 3966: 3967: 3968: 3969: 3970: 3971: 3972: 3973: 3974: 3975: 3976: 3977: 3978: 3979: 3980: 3981: 3982: 3983: 3984: 3985: 3986: 3987: 3988: 3989: 3990: 3991: 3992: 3993: 3994: 3995: 3996: 3997: 3998: 3999: 4000: 4001: 4002: 4003: 4004: 4005: 4006: 4007: 4008: 4009: 4010: 4011: 4012: 4013: 4014: 4015: 4016: 4017: 4018: 4019: 4020: 4021: 4022: 4023: 4024: 4025: 4026: 4027: 4028: 4029: 4030: 4031: 4032: 4033: 4034: 4035: 4036: 4037: 4038: 4039: 4040: 4041: 4042: 4043: 4044: 4045: 4046: 4047: 4048: 4049: 4050: 4051: 4052: 4053: 4054: 4055: 4056: 4057: 4058: 4059: 4060: 4061: 4062: 4063: 4064: 4065: 4066: 4067: 4068: 4069: 4070: 4071: 4072: 4073: 4074: 4075: 4076: 4077: 4078: 4079: 4080: 4081: 4082: 4083: 4084: 4085: 4086: 4087: 4088: 4089: 4090: 4091: 4092: 4093: 4094: 4095: 4096: 4097: 4098: 4099: 4100: 4101: 4102: 4103: 4104: 4105: 4106: 4107: 4108: 4109: 4110: 4111: 4112: 4113: 4114: 4115: 4116: 4117: 4118: 4119: 4120: 4121: 4122: 4123: 4124: 4125: 4126: 4127: 4128: 4129: 4130: 4131: 4132: 4133: 4134: 4135: 4136: 4137: 4138: 4139: 4140: 4141: 4142: 4143: 4144: 4145: 4146: 4147: 4148: 4149: 4150: 4151: 4152: 4153: 4154: 4155: 4156: 4157: 4158: 4159: 4160: 4161: 4162: 4163: 4164: 4165: 4166: 4167: 4168: 4169: 4170: 4171: 4172: 4173: 4174: 4175: 4176: 4177: 4178: 4179: 4180: 4181: 4182: 4183: 4184: 4185: 4186: 4187: 4188: 4189: 4190: 4191: 4192: 4193: 4194: 4195: 4196: 4197: 4198: 4199: 4200: 4201: 4202: 4203: 4204: 4205: 4206: 4207: 4208: 4209: 4210: 4211: 4212: 4213: 4214: 4215: 4216: 4217: 4218: 4219: 4220: 4221: 4222: 4223: 4224: 4225: 4226: 4227: 4228: 4229: 4230: 4231: 4232: 4233: 4234: 4235: 4236: 4237: 4238: 4239: 4240: 4241: 4242: 4243: 4244: 4245: 4246: 4247: 4248: 4249: 4250: 4251: 4252: 4253: 4254: 4255: 4256: 4257: 4258: 4259: 4260: 4261: 4262: 4263: 4264: 4265: 4266: 4267: 4268: 4269: 4270: 4271: 4272: 4273: 4274: 4275: 4276: 4277: 4278: 4279: 4280: 4281: 4282: 4283: 4284: 4285: 4286: 4287: 4288: 4289: 4290: 4291: 4292: 4293: 4294: 4295: 4296: 4297: 4298: 4299: 4300: 4301: 4302: 4303: 4304: 4305: 4306: 4307: 4308: 4309: 4310: 4311: 4312: 4313: 4314: 4315: 4316: 4317: 4318: 4319: 4320: 4321: 4322: 4323: 4324: 4325: 4326: 4327: 4328: 4329: 4330: 4331: 4332: 4333: 4334: 4335: 4336: 4337: 4338: 4339: 4340: 4341: 4342: 4343: 4344: 4345: 4346: 4347: 4348: 4349: 4350: 4351: 4352: 4353: 4354: 4355: 4356: 4357: 4358: 4359: 4360: 4361: 4362: 4363: 4364: 4365: 4366: 4367: 4368: 4369: 4370: 4371: 4372: 4373: 4374: 4375: 4376: 4377: 4378: 4379: 4380: 4381: 4382: 4383: 4384: 4385: 4386: 4387: 4388: 4389: 4390: 4391: 4392: 4393: 4394: 4395: 4396: 4397: 4398: 4399: 4400: 4401: 4402: 4403: 4404: 4405: 4406: 4407: 4408: 4409: 4410: 4411: 4412: 4413: 4414: 4415: 4416: 4417: 4418: 4419: 4420: 4421: 4422: 4423: 4424: 4425: 4426: 4427: 4428: 4429: 4430: 4431: 4432: 4433: 4434: 4435: 4436: 4437: 4438: 4439: 4440: 4441: 4442: 4443: 4444: 4445: 4446: 4447: 4448: 4449: 4450: 4451: 4452: 4453: 4454: 4455: 4456: 4457: 4458: 4459: 4460: 4461: 4462: 4463: 4464: 4465: 4466: 4467: 4468: 4469: 4470: 4471: 4472: 4473: 4474: 4475: 4476: 4477: 4478: 4479: 4480: 4481: 4482: 4483: 4484: 4485: 4486: 4487: 4488: 4489: 4490: 4491: 4492: 4493: 4494: 4495: 4496: 4497: 4498: 4499: 4500: 4501: 4502: 4503: 4504: 4505: 4506: 4507: 4508: 4509: 4510: 4511: 4512: 4513: 4514: 4515: 4516: 4517: 4518: 4519: 4520: 4521: 4522: 4523: 4524: 4525: 4526: 4527: 4528: 4529: 4530: 4531: 4532: 4533: 4534: 4535: 4536: 4537: 4538: 4539: 4540: 4541: 4542: 4543: 4544: 4545: 4546: 4547: 4548: 4549: 4550: 4551: 4552: 4553: 4554: 4555: 4556: 4557: 4558: 4559: 4560: 4561: 4562: 4563: 4564: 4565: 4566: 4567: "
     ]
    }
   ],
   "source": [
    "capture_eegno = r'(?:EEG TYPE:)\\s*(?P<eegno>.+?)(?:history|report|clinical|condition|patient|location)'\n",
    "capture_eegno1 = r'(?:Date:|Study date |Study date:)\\s*(?P<eegno>(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)\\s*[,\\s\\d/-]+)(?:[a-z]+)'\n",
    "capture_eegno2 = r'(?<!DOB:  )(?P<eegno>[\\d]+/[\\d]+/[\\d]+)' #DOB:  10/04/1993   \n",
    "eegDateRange = r'(?P<eegno>[\\d/]+-[\\d/]+)'\n",
    "eegDateStrict = r'(?P<eegno>[\\d]+/[\\d]+/[\\d]+)'\n",
    "#capture_eegno = r'(?P<eegno>[0-9]+)\\s*'\n",
    "re_eegno = re.compile(capture_eegno, re.DOTALL|re.IGNORECASE)\n",
    "re_eegno1 = re.compile(capture_eegno1, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno2 = re.compile(capture_eegno2, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateRange = re.compile(eegDateRange, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateStrict = re.compile(eegDateStrict, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "\n",
    "out_file = open(\"lpch_eeg_reports_interp_date_impression_type.csv\",'w')\n",
    "\n",
    "def detectType(eeg_no):\n",
    "    if re.search(\"ambulatory\", eeg_no, re.IGNORECASE):\n",
    "        return \"amb\"\n",
    "    if re.search(\"routine\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"routine\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"portable\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"spot\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"out\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"sleep\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"continuous\", eeg_no, re.IGNORECASE):\n",
    "        return \"ltm\"\n",
    "    if re.search(\"video\", eeg_no, re.IGNORECASE):\n",
    "        return \"ltm\"\n",
    "    if re.search(\"intraoperative\", eeg_no, re.IGNORECASE):\n",
    "        return \"iom\"\n",
    "    if re.search(\"electrocort\", eeg_no, re.IGNORECASE):\n",
    "        return \"iom\"\n",
    "    if re.search(\"amplitude\", eeg_no, re.IGNORECASE):\n",
    "        return \"aeeg\"\n",
    "    if re.search(\"aeeg\", eeg_no, re.IGNORECASE):\n",
    "        return \"aeeg\"\n",
    "    return \"\"\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_date_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    outfieldnames = reader.fieldnames\n",
    "    outfieldnames.append('type')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='***')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for line in reader:\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        print(str(i) + \": \", end='')\n",
    "        i += 1\n",
    "        #print(str(i) + \": \" + str(type(m)), end='')\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            eeg_no = detectType(eeg_no)\n",
    "            \n",
    "        if (len(eeg_no) == 0):\n",
    "            eeg_no = detectType(line['note'])\n",
    "\n",
    "        line['type'] = eeg_no\n",
    "        \n",
    "        writer.writerow(line)\n",
    "        #if (i>1556): \n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected error: <type 'exceptions.ValueError'>\n",
      "Unexpected error: <type 'exceptions.ValueError'>0/01/1975\n",
      "Unexpected error: <type 'exceptions.ValueError'>4/6-7/2012\n",
      "Unexpected error: <type 'exceptions.ValueError'>\n",
      "Unexpected error: <type 'exceptions.ValueError'>11/07-8/2013\n",
      "Unexpected error: <type 'exceptions.ValueError'>316501658\n",
      "Unexpected error: <type 'exceptions.ValueError'>318488965\n",
      "Unexpected error: <type 'exceptions.ValueError'>6/30//2014\n",
      "Unexpected error: <type 'exceptions.ValueError'>7/24-25/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>8/6-8/7/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>8/12-8/14/15\n",
      "Unexpected error: <type 'exceptions.ValueError'>9/1-9/3/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>9/1-9/2/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>9/2-9/3/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>9/2-9/3/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>8/31-9/2/15\n",
      "Unexpected error: <type 'exceptions.ValueError'>9/8-9/9/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>9/8-9/9/2015\n",
      "Unexpected error: <type 'exceptions.ValueError'>1/3-1/4/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>1/3-1/4/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>1/7-1/8/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>1/8-1/9/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>2/7-2/8/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>2/17-2/18/16\n",
      "Unexpected error: <type 'exceptions.ValueError'>4/4-4/5/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>5/2-5/3/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>5/3-5/4/2016\n",
      "Unexpected error: <type 'exceptions.ValueError'>5/31-6/2/16\n",
      "Unexpected error: <type 'exceptions.ValueError'>6/2-6/4/2016\n"
     ]
    }
   ],
   "source": [
    "# in order to load a corpus, reports must be in separate files\n",
    "from dateutil import parser\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_date_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    \n",
    "    for line in reader:\n",
    "        date_str= line['date']\n",
    "        try:\n",
    "            dateObj = parser.parse(date_str)\n",
    "            #print(dateObj.date())\n",
    "            dateFile = (str(dateObj.date()))\n",
    "            f = open(\"reports/\" + dateFile + \".txt\", \"a+\")\n",
    "            f.write(line['note'])\n",
    "            f.close()\n",
    "        except:\n",
    "            print(\"Unexpected error:\", str(sys.exc_info()[0]) + date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "#fileids = os.listdir(\"reports\")\n",
    "corpus_root = 'reports'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:10])\n",
    "           for fileid in wordlists.fileids()\n",
    "           for w in wordlists.words(fileid)\n",
    "           for target in ['grda', 'gpd']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['grda','gpd'], samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see if there are natural clusters, unsupervised\n",
    "#ngram of word, each one is dimension of word, then \n",
    "#applied text analysis python machine learning approaches on slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try tagging \n",
    "\n",
    "textAll_tokenized = list()\n",
    "with open(\"lpch_eeg_reports_interp_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        #print(line['note'])\n",
    "        text_raw = line['note'].lower()\n",
    "        text_tokenized = nltk.tokenize.word_tokenize(text_raw)\n",
    "        textAll_tokenized += text_tokenized\n",
    "        #words = list(text_raw.split())\n",
    "        #print(words)\n",
    "        #break\n",
    "\n",
    "text = nltk.Text(textAll_tokenized)\n",
    "text_tagged = nltk.pos_tag(text)\n",
    "\n",
    "        #print(text[10:200])\n",
    "print(text.similar('alpha'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which of these tags are the most common\n",
    "\n",
    "Tag\tMeaning\tEnglish Examples\n",
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADP\tadposition\ton, of, at, with, by, into, under\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CONJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "PRON\tpronoun\the, their, her, its, my, I, us\n",
    "VERB\tverb\tis, say, told, given, playing, would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('NN.*')\n",
    "nltk.help.upenn_tagset('JJ.*')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in text_tagged)\n",
    "tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tag_pairs = nltk.bigrams(text_tagged)\n",
    "#i = 0\n",
    "#for (a, b) in word_tag_pairs:\n",
    "#    print(b[1])\n",
    "#    i += 1\n",
    "#    if i>22:\n",
    "#        break\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'JJ']\n",
    "#print(noun_preceders)\n",
    "fdist = nltk.FreqDist(noun_preceders)\n",
    "[tag for (tag, _) in fdist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what are teh most common verbs\n",
    "word_tag_fd = nltk.FreqDist(text_tagged)\n",
    "[wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# most likely words for a given tag VBN = \n",
    "cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in text_tagged)\n",
    "list(cfd2['VBN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find vbd (past tense) and VBN (past participle), find words which can be both and see surrounding text\n",
    "nltk.help.upenn_tagset('VB.*')\n",
    "cfd1 = nltk.ConditionalFreqDist(text_tagged)\n",
    "[w for w in cfd1.conditions() if 'VBD' in cfd1[w] and 'VBN' in cfd1[w]]\n",
    "idx1 = text_tagged.index(('localized', 'VBD'))\n",
    "print(text_tagged[idx1-4:idx1+1])\n",
    "idx2 = text_tagged.index(('localized', 'VBN'))\n",
    "print(text_tagged[idx2-4:idx2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
