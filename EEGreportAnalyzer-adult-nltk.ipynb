{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert from tsv to csv\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "\n",
    "file_name = \"messer_SHC_note_final_2.tsv\"    \n",
    "out_file = open(\"SHC_note2.csv\",'w')\n",
    "i=0\n",
    "with open(file_name,'rb') as tsvin, open('new.csv', 'wb') as csvout:\n",
    "    tsvin = csv.reader(tsvin, delimiter='\\t')\n",
    "    csvout = csv.writer(csvout)\n",
    "\n",
    "    for row in tsvin:\n",
    "        #print(type(row))\n",
    "        if len(row) > 0:\n",
    "            csvout.writerow(row)\n",
    "        #i += 1\n",
    "        #if i > 3:\n",
    "        #    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#get rid of progress notes and others\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_RAW.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_FILTERED.csv\",'w')\n",
    "i=1\n",
    "        \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    outfieldnames = reader.fieldnames \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        if (line['Note'] is None) or (len(line['Note']) == 0):\n",
    "            print(\"malformed at %s\", i)\n",
    "            continue\n",
    "        notebody = line['Note'].lower()\n",
    "        notetype = line['Note_type_Desc'].lower()\n",
    "        if \"EEG Number\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"standard international system 10-20\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"standard international 10-20\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"conditions of recording\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"conditions of the recording\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"eeg #\" in notebody:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "            \n",
    "        if \"neurodiagnostic\" in notetype:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        if \"outpatient procedure\" in notetype:\n",
    "            writer.writerow(line)\n",
    "            continue\n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + notebody[:100], i )\n",
    "        #print()\n",
    "        #if i > 1900:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #i += 1\n",
    "    #if (i>600):\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# normal versus abn\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "import re\n",
    "import string\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_FILTERED.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_IMPRESS.csv\",'w')\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation:\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegno = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:|Impression|impression:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:|SUMMARY|Summary)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegnoLoose = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "\n",
    "\n",
    "def findTrueImpression(eeg_no,line,i):\n",
    "    wordlist = eeg_no.lower().split()\n",
    "    words = [''.join(c for c in s if c not in string.punctuation) for s in wordlist]\n",
    "    #print(words)\n",
    "    if \"normal\" in words and \"abnormal\" in words:\n",
    "        #equivocal\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i) + \"EQUIVOCAL\"+  eeg_no)\n",
    "            print()\n",
    "            \n",
    "    elif \"abnormal\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"status\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"seizure\" in words and \"nonepileptic\" in words:\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i)  +\"UNKNOWN-\"+ eeg_no)\n",
    "            print()\n",
    "    elif \"nonepileptic\" in words:\n",
    "            line['impression'] = \"normal\"\n",
    "    elif \"seizure\" in words:\n",
    "            line['impression'] = \"abnormal\"\n",
    "    elif \"slowing\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"normal\" in words:\n",
    "        line['impression'] = \"normal\"\n",
    "    else: \n",
    "        line['impression'] = eeg_no\n",
    "    #print(\"(\" + repr(i) + \")\" + line['impression'] + \"-\" + eeg_no.lower())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def findSpecificPhrases(eeg_no,line,i):\n",
    "    if \"This EEG is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"\n",
    "    elif \"is normal for age\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the broad normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG recording was normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This record is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"  \n",
    "    elif \"EEG recording is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This is a normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "         \n",
    "\n",
    "    elif \"This EEG is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"           \n",
    "    elif \"is abnormal because of\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"     \n",
    "    elif \"recording is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"markedly abnormal\" in eeg_no.lower():\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"is abnormal due to\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This record is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This is an abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "        \n",
    "\n",
    "    else:\n",
    "        return False        \n",
    "\n",
    "        \n",
    "i=1        \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    outfieldnames = reader.fieldnames \n",
    "    outfieldnames.append('impression')     \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            findTrueImpression(eeg_no,line,i)\n",
    "        else:\n",
    "            #try a looser find that may introduce more false information (more sensitive less specific pattern)\n",
    "            m = re_eegnoLoose.search(line['note'])\n",
    "            if m:\n",
    "                eeg_no = m.group('eegno')\n",
    "                findTrueImpression(eeg_no,line,i)\n",
    "            elif \"preliminary\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            elif \"prelim\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            else:\n",
    "                print(\"###IMPRESSION BLOCK MISSING-\" + repr(i) + line['note'])\n",
    "                print()\n",
    "\n",
    "        writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + notebody[:100], i )\n",
    "        #print()\n",
    "        #if i > 4446:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normal versus abn AND remove impression block from notebody\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import csvkit as csv\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "file_name = \"SHC_ALL_NOTES_FILTERED.csv\"    \n",
    "out_file = open(\"SHC_ALL_NOTES_IMPRESS.csv\",'w')\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation:\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegno = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "eegImprPattern = r'(?:\\\n",
    "Impression and Summary:|IMPRESSION and SUMMARY:|IMPRESSION and CONCLUSIONS:\\\n",
    "|IMPRESSION|IMPRESSIONS|IMPRESSION:|Impression:|IMPRESSIONS:|Impression|impression:\\\n",
    "|INTERPRETATIONS:|INTERPRETATION:|Interpretation\\\n",
    "|CONCLUSION:|CONCLUSIONS:|Conclusion:\\\n",
    "|Summary:|SUMMARY:|SUMMARY|Summary)\\s*(?P<eegno>[\\w\\d \\.\\-\\(\\),]+)'\n",
    "re_eegnoLoose = re.compile(eegImprPattern, re.DOTALL|re.MULTILINE)\n",
    "\n",
    "eegTypePattern = r'[a-z]{1}[0-9]+\\-[0-9]+'\n",
    "re_eegnoType = re.compile(eegTypePattern, re.DOTALL|re.IGNORECASE)\n",
    "\n",
    "eegDurationPattern = r'Duration:\\s*([\\d]+:[\\d]+:[\\d]+)'\n",
    "re_eegnoDuration = re.compile(eegDurationPattern, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern2 = r'Duration of Study:\\s*(([\\d]+)\\s*hour|([\\d]+)\\s*day)'\n",
    "re_eegnoDuration2 = re.compile(eegDurationPattern2, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern3 = r'test dates:\\s*([\\d\\/]+)\\s*-\\s*([\\d\\/]+)' \n",
    "re_eegnoDuration3 = re.compile(eegDurationPattern3, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern4 = r'\\s*([\\d\\.]+)\\s*(hour(s?)|day(s?))' \n",
    "re_eegnoDuration4 = re.compile(eegDurationPattern4, re.DOTALL|re.IGNORECASE)\n",
    "eegDurationPattern5 = r'test dates:\\s*([\\d\\/]+).*?-\\s*([\\d\\/]+)' \n",
    "re_eegnoDuration5 = re.compile(eegDurationPattern5, re.DOTALL|re.IGNORECASE)\n",
    "\n",
    "eegAbnormalityTypePattern = ['no epileptiform', 'absence of epileptiform', 'not epileptiform']\n",
    "#r'(?<!no)\\s+(epileptiform)'\n",
    "eegAbnormalityTypePattern2 = r'(?<!absence of)\\s+(epileptiform)'\n",
    "#(?:(?!\\bno\\b).)*?\\bepileptiform\\b'\n",
    "#re_eegnoAbnormalityType1 = re.compile(eegAbnormalityTypePattern1, re.DOTALL|re.IGNORECASE|re.MULTILINE)\n",
    "\n",
    "def findTrueImpression(eeg_no,line,i):\n",
    "    wordlist = eeg_no.lower().split()\n",
    "    words = [''.join(c for c in s if c not in string.punctuation) for s in wordlist]\n",
    "    #print(words)\n",
    "    if \"normal\" in words and \"abnormal\" in words:\n",
    "        #equivocal\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i) + \"EQUIVOCAL\"+  eeg_no)\n",
    "            print()\n",
    "            \n",
    "    elif \"abnormal\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"status\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"seizure\" in words and \"nonepileptic\" in words:\n",
    "        result = findSpecificPhrases(eeg_no,line,i)\n",
    "        if (result == False):\n",
    "            line['impression'] = \"unknown\"\n",
    "            print(repr(i)  +\"UNKNOWN-\"+ eeg_no)\n",
    "            print()\n",
    "    elif \"nonepileptic\" in words:\n",
    "            line['impression'] = \"normal\"\n",
    "    elif \"seizure\" in words:\n",
    "            line['impression'] = \"abnormal\"\n",
    "    elif \"slowing\" in words:\n",
    "        line['impression'] = \"abnormal\"\n",
    "    elif \"normal\" in words:\n",
    "        line['impression'] = \"normal\"\n",
    "    else: \n",
    "        line['impression'] = eeg_no\n",
    "    #print(\"(\" + repr(i) + \")\" + line['impression'] + \"-\" + eeg_no.lower())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def findSpecificPhrases(eeg_no,line,i):\n",
    "    if \"This EEG is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"\n",
    "    elif \"is normal for age\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG is within the broad normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"EEG recording was normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This record is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"  \n",
    "    elif \"EEG recording is normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "    elif \"This is a normal\" in eeg_no:\n",
    "        line['impression'] = \"normal\"            \n",
    "         \n",
    "\n",
    "    elif \"This EEG is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"           \n",
    "    elif \"is abnormal because of\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"     \n",
    "    elif \"recording is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"markedly abnormal\" in eeg_no.lower():\n",
    "        line['impression'] = \"abnormal\"      \n",
    "    elif \"is abnormal due to\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This record is abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "    elif \"This is an abnormal\" in eeg_no:\n",
    "        line['impression'] = \"abnormal\"  \n",
    "        \n",
    "\n",
    "    else:\n",
    "        return False        \n",
    "\n",
    "\n",
    "def setImpressionBody(m,line,i):\n",
    "    impstr = line['note'][m.start():]\n",
    "    n = re.search(r'(?:comments|comment|clinical correlation|DETAILED FINDINGS)', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        line['impressionBody'] = impstr[:n.start()]\n",
    "    else:\n",
    "        line['impressionBody'] = impstr\n",
    "\n",
    "    # get rid of double spaces\n",
    "    line['impressionBody'] = \" \".join(line['impressionBody'].split())\n",
    "\n",
    "def setImpressionType(line,i):\n",
    "    line['impressionType'] = \"\"        \n",
    "    impstr = line['impressionBody']\n",
    "    if impstr is None:\n",
    "        return\n",
    "    #https://regex101.com/\n",
    "\n",
    "    n = re.search(r'(?:(?<!absence of)(?<!no)(?<!not))\\s+(epileptiform)', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        if (\"other epileptiform\" not in impstr.lower()):\n",
    "            line['impressionType'] += ' ed' #epileptiform discharge\n",
    "\n",
    "    n = re.search(r'(?:(?<!absence of)(?<!no)(?<!not))\\s+(discharge|spike)', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        line['impressionType'] += ' dc' #discharges which are more questionable\n",
    "        \n",
    "    n = re.search(r'seizure', impstr, re.IGNORECASE)\n",
    "    if n:\n",
    "        if (\"no evidence for electrographic seizures\" not in impstr.lower()) and \\\n",
    "           (\"for a seizure disorder\" not in impstr.lower()) and \\\n",
    "           (\"no evidence for electrographic seizure\" not in impstr.lower()) and \\\n",
    "           (\"does not exclude a clinical diagnosis of seizure\" not in impstr.lower()) and \\\n",
    "           (\"no electrographic seizure\" not in impstr.lower()):\n",
    "            line['impressionType'] += ' sz'\n",
    "\n",
    "def durationByDates(t1, t2):\n",
    "    dtformat = '%m/%d/%Y'\n",
    "    try:\n",
    "        t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "        t2 = datetime.datetime.strptime(t2, dtformat)\n",
    "        t1 = t1.date()\n",
    "        t2 = t2.date()\n",
    "        td = t2-t1\n",
    "        return (repr(td.days*24) +\":\"+ repr(td.seconds//3600).zfill(2)  +\":\"+ repr((td.seconds//60)%60).zfill(2) )\n",
    "    except:\n",
    "        return \"\"\n",
    "#print(datetime.timedelta(0, 8, 562000))\n",
    "#divmod(b.days * 86400 + b.seconds, 60)\n",
    "#datetime.timedelta(0, 8, 562000)\n",
    "\n",
    "def durationByHoursHopefully(eegReport): #last ditch effort to match any mention of days or hours in the report\n",
    "    hours = 0 \n",
    "    days = 0\n",
    "    i=0\n",
    "    for match in re.finditer(eegDurationPattern4, eegReport):\n",
    "        matchtxt = match.group(2)\n",
    "        i += 1\n",
    "        #print (\"hi\" + repr(i) + match.group(1) + matchtxt)\n",
    "        if \"hour\" in matchtxt:\n",
    "            hours =  int(float(match.group(1)))\n",
    "        elif \"day\" in matchtxt:\n",
    "            days = int(float(match.group(1)))\n",
    "            #print(\"################################\")\n",
    "    totalhours=  hours + 24*days\n",
    "    #print (totalhours)\n",
    "    return repr(totalhours) + ':00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n"
     ]
    }
   ],
   "source": [
    "from spacy.en import English\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import Tree\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "spacyParser = English()\n",
    "\n",
    "def splitCompoundSentences(paragraph):\n",
    "    parsedData = spacyParser(paragraph)\n",
    "    #sentences = [sent.string.strip() for sent in doc]\n",
    "\n",
    "    # Let's look at the sentences\n",
    "    sents = \"\"\n",
    "    # the \"sents\" property returns spans\n",
    "    # spans have indices into the original string\n",
    "    # where each index value represents a token\n",
    "    for span in parsedData.sents:\n",
    "        # go from the start to the end of each span, returning each token in the sentence\n",
    "        # combine each token using join()\n",
    "        sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "        subj_flag = False\n",
    "        verb_flag = False\n",
    "        compound_flag = False\n",
    "        first = True\n",
    "        candidate_sent = \"\" \n",
    "        i = 0\n",
    "        lastPeriod = 0\n",
    "        for token in span[:-1]:\n",
    "            #print(str(token.orth_), str(token.pos_))\n",
    "            if token.pos_==\"VERB\" and (subj_flag==True):\n",
    "                verb_flag = True\n",
    "                #print(\"verb:\"+str(token.orth_))\n",
    "            if (token.pos_==\"NOUN\"):\n",
    "                subj_flag = True\n",
    "                #print(\"subj:\"+str(token.orth_))\n",
    "            if token.pos_==\"PUNCT\":            \n",
    "                if (subj_flag==True) and (verb_flag==True):\n",
    "                    # compound sentence detected; split the sentence\n",
    "                    subj_flag = False\n",
    "                    verb_flag = False\n",
    "                    candidate_sent = str(span[lastPeriod:i-1]) + \".\"\n",
    "                    #candidate_sent = candidate_sent[0:len(candidate_sent)-1]\n",
    "                    lastPeriod = i+1\n",
    "                    #candidate_sent += (\".\")\n",
    "                    sents += candidate_sent + \"\\n\"\n",
    "                    #candidate_sent = \"\"        \n",
    "\n",
    "            i += 1\n",
    "            #candidate_sent += str(token.orth_) + \" \"\n",
    "\n",
    "            #print(candidate_sent)\n",
    "\n",
    "            #print(\"HI\" + str(candidate_sent))\n",
    "        candidate_sent = str(span[lastPeriod:]) \n",
    "        #candidate_sent = candidate_sent[0:len(candidate_sent)-1]\n",
    "        #candidate_sent += str(span[-1])\n",
    "        sents += candidate_sent + \"\\n\"\n",
    "\n",
    "    # flatten the list into a paragraph for nltk\n",
    "    #paragraph = [item for sublist in sents for item in sublist]\n",
    "    #paragraph = ' '.join(map(str, paragraph))\n",
    "    #print(sents)     \n",
    "    return sents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Splitter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        text = splitCompoundSentences(text)\n",
    "        text = text.replace(\"There\", \". There\").replace(\"The\", \". The\")\n",
    "        sentences = self.nltk_splitter.tokenize(text)        \n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences] \n",
    "        # instead, split compound sentences to 2 sentences\n",
    "        #pprint(tokenized_sentences )\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "\n",
    "    def __init__(self, dictionary_paths):\n",
    "        files = [open(path, 'r') for path in dictionary_paths]        \n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        for file in files:\n",
    "            file.close() \n",
    "        #map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "        #self.closure(files)\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "    \n",
    "    #def __del__(self):\n",
    "        #print(\"closing dict\")\n",
    "        #self.closure\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentence_score_deprecated(sentence_tokens, previous_token, acum_score):    \n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentence_score(sentences):    \n",
    "    scored_sentences = []\n",
    "    if not sentences:\n",
    "        return None\n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            tagged_expression = []\n",
    "            token_score = 0\n",
    "            for (word, wordstem, postag) in sentence:\n",
    "                if 'inc' in postag:\n",
    "                    token_score *= 2.0\n",
    "                elif 'dec' in postag:\n",
    "                    token_score /= 2.0\n",
    "                elif 'inv' in postag:\n",
    "                    token_score += -1             \n",
    "                else:\n",
    "                    token_score = 0\n",
    "                tagged = (word, wordstem, postag, token_score)\n",
    "                tagged_expression.append(tagged)\n",
    "            scored_sentences.append(tagged_expression)\n",
    "    return scored_sentences\n",
    "\n",
    "def sentence_simplify(sentences):    \n",
    "    scored_sentences = []\n",
    "    if not sentences:\n",
    "        return None\n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            tagged_expression = []\n",
    "            for (word, wordstem, postag) in sentence:\n",
    "                #print(postag)\n",
    "                if ('DT' in postag) or \\\n",
    "                    ('IN' in postag) or \\\n",
    "                    ('NN' in postag) or \\\n",
    "                    ('RB' in postag) or \\\n",
    "                    ('JJ' in postag) or \\\n",
    "                    ('NNS' in postag):\n",
    "                    tagged = (word, wordstem, postag)\n",
    "                    tagged_expression.append(tagged)\n",
    "                    #print('ok')\n",
    "            if tagged_expression:\n",
    "                scored_sentences.append(tagged_expression)\n",
    "    return scored_sentences\n",
    "\n",
    "\n",
    "def sentiment_score(review):\n",
    "    finalized_score = 0\n",
    "    if (review is None):\n",
    "        return 0\n",
    "    for sentence in review:\n",
    "        if sentence:\n",
    "            #print()\n",
    "            #pprint(sentence)\n",
    "            token_score = 0\n",
    "            for (word, wordstem, tags, score) in sentence:\n",
    "                token_score += sum([value_of(tag) for tag in tags])\n",
    "            #    print(token_score)\n",
    "            for (word, wordstem, tags, score) in sentence:\n",
    "                if score != 0:\n",
    "                    token_score *= score \n",
    "            #        print(word + str(score) +\"-->\"+str(token_score))\n",
    "            #in examples of 'there are seizures. there are events with no ictal pattern', we have to ignore the next sentence\n",
    "            #print(\"score is now \" + str(token_score))\n",
    "            if (token_score<1):\n",
    "                finalized_score += token_score\n",
    "            #    print(\"final: \" + str(finalized_score))\n",
    "    return finalized_score\n",
    "\n",
    "        \n",
    "def sentimentAnalysisForSeizure(text):\n",
    "\n",
    "    splitter = Splitter()\n",
    "    postagger = POSTagger()\n",
    "    dicttagger = DictionaryTagger([ 'dicts/seizure/positive.yml', 'dicts/seizure/negative.yml', \n",
    "                                    'dicts/seizure/inc.yml', 'dicts/seizure/dec.yml', 'dicts/seizure/inv.yml'])\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        splitted_sentences = splitter.split(text)\n",
    "    else:\n",
    "        return 0\n",
    "    #pprint(text)\n",
    "    #pprint(splitted_sentences)\n",
    "\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint(pos_tagged_sentences)\n",
    "\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    #pprint(dict_tagged_sentences)\n",
    "\n",
    "    dict_tagged_sentences = sentence_simplify(dict_tagged_sentences)\n",
    "    #print(\"simplified:\", \"\")\n",
    "    #print(dict_tagged_sentences)\n",
    "    #() + 1\n",
    "    score_tagged_sentences = sentence_score(dict_tagged_sentences)\n",
    "    #print(\"scored:\", \"\")\n",
    "    #pprint(score_tagged_sentences)\n",
    "    #print(\"analyzing sentiment...\")\n",
    "    score = sentiment_score(score_tagged_sentences)\n",
    "    #print(score)    \n",
    "    return score\n",
    "\n",
    "def sentimentAnalysisForEpileptiform(text):\n",
    "\n",
    "    splitter = Splitter()\n",
    "    postagger = POSTagger()\n",
    "    dicttagger = DictionaryTagger([ 'dicts/epileptiform/positive.yml', 'dicts/epileptiform/negative.yml', \n",
    "                                    'dicts/epileptiform/inc.yml', 'dicts/epileptiform/dec.yml', 'dicts/epileptiform/inv.yml'])\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = text.replace(\"-\", \" \")\n",
    "        splitted_sentences = splitter.split(text)\n",
    "    else:\n",
    "        return 0\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    #pprint(pos_tagged_sentences)\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    del dicttagger\n",
    "    dict_tagged_sentences = sentence_simplify(dict_tagged_sentences)\n",
    "    score_tagged_sentences = sentence_score(dict_tagged_sentences)\n",
    "    #print(\"scored:\", \"\")\n",
    "    #pprint(score_tagged_sentences)\n",
    "    score = sentiment_score(score_tagged_sentences)\n",
    "    return score\n",
    "\n",
    "\n",
    "#sentimentAnalysisForSeizure(\"IMPRESSION: This EEG capturing wakefulness and sleep is ABNORMAL due to: 1) mild diffuse slowing 2) multifocal and generalized spikes and spike-and-wave complexes (including slow spike wave) and occasional diffuse paroxysmal fast activity 3) ~4 brief <20 second seizures/ictal patterns (without logged behaviors) Overall, given that the patient has had similarly abnormal EEGs in the remote past, occurrence of four <20 second does not explain decreased oral intake and increased sleepiness.\")\n",
    "sentimentAnalysisForEpileptiform(\"IMPRESSION: This is an abnormal ambulatory EEG due to bursts of generalized polyspike and polyspike wave. There are no prolonged runs or seizures during this recording.\")\n",
    "#sentimentAnalysisForEpileptiform(\"INTERPRETATION: This is an abnormal study due to the presence of occasional to frequent bursts of slowing, mostly diffuse and rarely more lateralized to the left or right hemisphere. There were no clear epileptiform discharges or seizures. Comments The bursts of slowing are overall non-specific with regards to etiology. This finding may indicate diffuse cerebral dysfunction. There is no clear indication of epileptiform discharges, although a rhythmic epileptiform abnormality related to the bursts of slowing cannot entirely be excluded. Absence of epileptiform discharges does not preclude a clinical diagnosis of epilepsy or seizures. Clinical correlation is advised.\")\n",
    "#sentimentAnalysisForSeizure(\"INTERPRETATION: Impression This is a normal awake and asleep EEG. There are no seizures or epileptiform discharges during this recording. Comments Absence of epileptiform discharges does not exclude a clinical diagnosis of seizures or epilepsy. There is no documentation of patient's typical events concerning for seizures on the log sheet.\")\n",
    "#sentimentAnalysis(\"there are no such things as ictal activity abnormal\")\n",
    "#sentimentAnalysis(\"there questionable activity abnormal\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "C:\\Users\\jkpython64\\Anaconda3\\envs\\py36\\lib\\importlib\\_bootstrap.py:205: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3 days, 0:36:17'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "def loadMasterCSVFile():\n",
    "    with codecs.open(\"NKdatabaseDump.csv\", 'r', encoding='utf-8', errors='ignore') as cf:\n",
    "        i = 0\n",
    "        masterCSV = csv.reader(cf)\n",
    "        masterCSV_list = [row for row in masterCSV]\n",
    "        print(len(masterCSV_list))\n",
    "        return masterCSV_list\n",
    "\n",
    "\n",
    "def durationByDatesForCSV(t1, t2):\n",
    "    try:\n",
    "        dtformat = '%m/%d/%Y %I:%M:%S %p'\n",
    "    #try:\n",
    "    #print(t1)\n",
    "        t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "        t2 = datetime.datetime.strptime(t2, dtformat)\n",
    "    #print(t2)\n",
    "    #t1 = t1.date()\n",
    "    #t2 = t2.date()\n",
    "    except ValueError:\n",
    "        try:\n",
    "            dtformat = '%m/%d/%Y'\n",
    "            t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "            t2 = datetime.datetime.strptime(t2, dtformat)        \n",
    "        except ValueError:\n",
    "            try:\n",
    "                dtformat = '%m/%d/%Y (%I:%M)'\n",
    "                t1 = datetime.datetime.strptime(t1, dtformat)\n",
    "                t2 = datetime.datetime.strptime(t2, dtformat)        \n",
    "            except:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    td = t2-t1\n",
    "    td = abs(td)\n",
    "    return str(td)\n",
    "    #print(td)\n",
    "    return (repr(td.days*24) +\":\"+ repr(td.seconds//3600).zfill(2)  +\":\"+ repr((td.seconds//60)%60).zfill(2) )\n",
    "    #except:\n",
    "    #    return \"\"\n",
    "    \n",
    "# return a dateDelta for any given exam number by looking up all the rows with an exam number, \n",
    "# then looking for the minimal and maximal dates within those rows\n",
    "def masterCSV_daterange(examNO):\n",
    "    #print(len(masterCSV_list))\n",
    "    #columns=masterCSV_list[0]\n",
    "    #print(columns)\n",
    "    df = pd.DataFrame(masterCSV_list[1:],\n",
    "                    columns=masterCSV_list[0])\n",
    "    #print(df)\n",
    "    #HTML(df.to_html())\n",
    "    #display(df)\n",
    "    #df = df.drop_duplicates()\n",
    "    po = df['INF_ScheduleExam@ExamNo_STR'].str.contains(examNO)\n",
    "    #display(po)\n",
    "    #idx = df[po].index.tolist()\n",
    "    #r = df.loc(idx)\n",
    "    #display(r)\n",
    "    resultingDF = df.iloc[np.flatnonzero(po)]\n",
    "    #display(r)\n",
    "    dateList = list()\n",
    "    dateList.extend(resultingDF['INF_ScheduleExam@StartTime_DT'].tolist())\n",
    "    dateList.extend(resultingDF['INF_ScheduleExam@EndTime_DT'].tolist())\n",
    "    #print(examNO)\n",
    "    #display(dateList)\n",
    "    if (len(dateList) is 0):\n",
    "        return None\n",
    "    max_value = max(dateList)\n",
    "    min_value = min(dateList)\n",
    "    #print(max_value)\n",
    "    #print(min_value)\n",
    "    #duration = max_value - min_value\n",
    "    duration = durationByDatesForCSV(min_value, max_value)\n",
    "    return duration\n",
    "    print(duration)\n",
    "    #mask = np.column_stack([df['INF_ScheduleExam@ExamNo_STR'].str.contains(examNO)])\n",
    "    #display(mask)\n",
    "    #examNo_col = [item[''] for item in masterCSV_list]\n",
    "    #a = np.array(masterCSV_list)\n",
    "    #i,j = np.where(a==examNO)\n",
    "    #x = enumerate(masterCSV_list) \n",
    "    #print(x.value)\n",
    "    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(masterCSV_list)))\n",
    "    #result = [i for i,x in enumerate(masterCSV_list) if x.==examNO] # => [1, 3]\n",
    "    #print(i,j)\n",
    "\n",
    "masterCSV_list = loadMasterCSVFile()\n",
    "masterCSV_daterange(\"A17-293\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "i=1              \n",
    "with open(file_name) as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    #optional offset \n",
    "    #for line in reader:\n",
    "    #    i += 1\n",
    "    #    if (i>9425):\n",
    "    #        i=1\n",
    "    #        break\n",
    "\n",
    "    outfieldnames = reader.fieldnames \n",
    "    outfieldnames.append('notebody')  \n",
    "    outfieldnames.append('examno')\n",
    "    outfieldnames.append('impressionBody')     \n",
    "    outfieldnames.append('impression')     \n",
    "    outfieldnames.append('notetype')     \n",
    "    outfieldnames.append('duration')     \n",
    "    outfieldnames.append('impressionType')     \n",
    "    outfieldnames.append('epileptiformScore')     \n",
    "    outfieldnames.append('seizureScore')     \n",
    "    outfieldnames.append('isSeizure')     \n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        #write the notebody and impression\n",
    "        if m:\n",
    "            #print(m.start())\n",
    "            #print(line['note'][:m.start()])\n",
    "            #break\n",
    "            line['notebody'] = line['note'][:m.start()]\n",
    "            eeg_no = m.group('eegno')\n",
    "            findTrueImpression(eeg_no,line,i)\n",
    "            setImpressionBody(m,line,i)\n",
    "        else:\n",
    "            #try a looser find that may introduce more false information (more sensitive less specific pattern)\n",
    "            m = re_eegnoLoose.search(line['note'])\n",
    "            if m:\n",
    "                line['notebody'] = line['note'][:m.start()]\n",
    "                #setImpressionBody(m,line,i)\n",
    "                eeg_no = m.group('eegno')\n",
    "                findTrueImpression(eeg_no,line,i)\n",
    "            elif \"preliminary\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            elif \"prelim\" in line['note'].lower():\n",
    "                line['impression'] = \"prelim\"\n",
    "            else:\n",
    "                print(\"###IMPRESSION BLOCK MISSING-\" + repr(i) + line['note'])\n",
    "                print()\n",
    "\n",
    "                  \n",
    "        # figure out the duration by cross-reference the duration of the study based on the examno\n",
    "        m = re_eegnoType.search(line['note'])\n",
    "        line['duration'] = None\n",
    "        if m:\n",
    "            #print(\"got csv table lookup\")\n",
    "            line['duration'] = masterCSV_daterange(m.group(0))\n",
    "            #print(line['duration'])\n",
    "        if line['duration'] is None:                \n",
    "            #write the duration by some other messier means\n",
    "            m = re_eegnoDuration.search(line['note'])\n",
    "            if m:            \n",
    "                match = m.group(1)\n",
    "                line['duration'] = match\n",
    "            else:\n",
    "                m = re_eegnoDuration2.search(line['note'])\n",
    "                if m:\n",
    "                    match = m.group(1)\n",
    "                    #print(match)\n",
    "                    if \"hour\" in match:\n",
    "                        line['duration'] =  m.group(2) + ':00:00'\n",
    "                    elif \"day\" in match:\n",
    "                        line['duration'] = repr(int(m.group(3))*24) + ':00:00'  \n",
    "                else:\n",
    "                    m = re_eegnoDuration3.search(line['note'])\n",
    "                    if m:\n",
    "                        line['duration'] = durationByDates(m.group(1), m.group(2))\n",
    "                    else:\n",
    "                        m = re_eegnoDuration4.search(line['note'])\n",
    "                        if m:\n",
    "                            line['duration'] = durationByHoursHopefully(line['note'])\n",
    "                        else:\n",
    "                            m = re_eegnoDuration5.search(line['note'])\n",
    "                            #print(\"here i am\" + repr(m))\n",
    "                            if m:\n",
    "                                print(m.group(1))\n",
    "                                line['duration'] = durationByDates(m.group(1), m.group(2))\n",
    "                            \n",
    "        #print(repr(i) + \": \" + repr(line['duration']))\n",
    "        #() + 1                                \n",
    "                    \n",
    "                            \n",
    "\n",
    "        #write the notetype\n",
    "        m = re_eegnoType.search(line['note'])\n",
    "        if m:\n",
    "            match = m.group(0)\n",
    "            match = match.lower()\n",
    "            line['examno'] = match\n",
    "            if (\"s\" in match):\n",
    "                line['notetype'] = \"spot\"\n",
    "            elif (\"v\" in match):\n",
    "                line['notetype'] = \"ceeg\"\n",
    "            elif (\"a\" in match):\n",
    "                line['notetype'] = \"ambu\"\n",
    "            elif (\"f\" in match):\n",
    "                line['notetype'] = \"spot inpt\"\n",
    "            elif (\"e\" in match):\n",
    "                line['notetype'] = \"ceeg\"\n",
    "            else:\n",
    "                line['notetype'] = \"unk\"\n",
    "                \n",
    "\n",
    "        #determine procedure type by duration\n",
    "        #make sure ambulatories are >24 \n",
    "        #\n",
    "        dtformat = '%H:%M:%S'\n",
    "        try:\n",
    "            t = datetime.datetime.strptime(line['duration'], dtformat)\n",
    "            totMinutes = t.minute + t.hour*60\n",
    "            #determine the unknown report based off the duration only\n",
    "            if line['duration'] is not None:\n",
    "                if line['notetype'] is None:\n",
    "                    if totMinutes < 100 and totMinutes > 18:\n",
    "                        line['notetype'] = \"spot?\"\n",
    "                        pass\n",
    "            if \"day\" not in line['duration']:\n",
    "                if \"ambu\" is line['notetype']:\n",
    "                    if totMinutes < 1320 and totMinutes > 1:\n",
    "                        line['notetype'] = \"longspot\"\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        #write the abnormality type\n",
    "        #setImpressionType(line,i)\n",
    "            \n",
    "        #write sentiment score\n",
    "        line['seizureScore'] = sentimentAnalysisForSeizure(line['impressionBody'])\n",
    "        line['epileptiformScore'] = sentimentAnalysisForEpileptiform(line['impressionBody'])\n",
    "\n",
    "        #write the abnormality type\n",
    "        line['impressionType'] = \"\"\n",
    "        if int(line['seizureScore']) < 0:\n",
    "            line['impressionType'] += ' sz' \n",
    "        if int(line['epileptiformScore']) < 0:\n",
    "            line['impressionType'] += ' ed' \n",
    "        \n",
    " \n",
    "            \n",
    "        #print(repr(i) + \": \"+ line['impressionType'])\n",
    "        \n",
    "        if (line['notetype'] is \"ambu\"):        \n",
    "            writer.writerow(line)    \n",
    "        #writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + line['note_Id'][:20] + repr(line['sentimentScore']), i )\n",
    "        #print(\"line %s\" + line['notebody'][:200], i )\n",
    "        #print()\n",
    "        #if i > 1555:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "#close(file_name)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create examno column     \n",
    "i=1              \n",
    "with open(\"ambuDraft1.csv\") as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    #optional offset \n",
    "    #for line in reader:\n",
    "    #    i += 1\n",
    "    #    if (i>9425):\n",
    "    #        i=1\n",
    "    #        break\n",
    "\n",
    "    outfieldnames = reader.fieldnames \n",
    "    outfieldnames.append('examno')\n",
    "    out_file.close()\n",
    "    out_file = open(\"ambuDraft2.csv\",'w')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='*') \n",
    "    writer.writeheader()\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        \n",
    "        #write the notetype\n",
    "        m = re_eegnoType.search(line['note'])\n",
    "        if m:\n",
    "            match = m.group(0)\n",
    "            match = match.lower()\n",
    "            line['examno'] = match\n",
    "                \n",
    "\n",
    "        writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + line['note_Id'][:20] + repr(line['sentimentScore']), i )\n",
    "        #print(\"line %s\" + line['notebody'][:200], i )\n",
    "        #print()\n",
    "        #if i > 1555:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "#close(file_name)\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the duration of exam versus the date of the exam\n",
    "i=1              \n",
    "with open(\"ambuDraft_duplicates_removed.csv\") as cf: \n",
    "    reader = csv.DictReader(cf) \n",
    "    #optional offset \n",
    "    #for line in reader:\n",
    "    #    i += 1\n",
    "    #    if (i>9425):\n",
    "    #        i=1\n",
    "    #        break\n",
    "\n",
    "    for line in reader:\n",
    "        i += 1\n",
    "        eeg_no=\"\"\n",
    "        \n",
    "        #write the notetype\n",
    "        m = re_eegnoType.search(line['note'])\n",
    "        if m:\n",
    "            match = m.group(0)\n",
    "            match = match.lower()\n",
    "            line['examno'] = match\n",
    "                \n",
    "\n",
    "        writer.writerow(line)    \n",
    "        #print(type(line['Note']))\n",
    "        #print(\"line %s\" + line['note_Id'][:20] + repr(line['sentimentScore']), i )\n",
    "        #print(\"line %s\" + line['notebody'][:200], i )\n",
    "        #print()\n",
    "        #if i > 1555:\n",
    "        #    break\n",
    "    #print(eeg_no)    \n",
    "    #if (i>600):\n",
    "    #    break\n",
    "#close(file_name)\n",
    "#writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import csvkit as csv\n",
    "\n",
    "allReports = \"\"\n",
    "with open(\"SHC_ALL_NOTES_FILTERED.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        allReports += line['note'] + \" \\n\\n\"\n",
    "        # print(desc)\n",
    "        # now manipulate the note body\n",
    "        \n",
    "f = open(\"allReports.txt\", \"w\")\n",
    "f.write(allReports)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a histogram of the most commonly used words in all EEG reports\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = open(\"allReports.txt\", \"r\")\n",
    "allReports = f.read() \n",
    "f.close()\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "allwords_stemmed = tokenize_and_stem(allReports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of stop words\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_words = [word for word in allwords_stemmed if word not in stopwords]\n",
    "\n",
    "letter_counts = Counter(filtered_words)\n",
    "df = pandas.DataFrame.from_dict(letter_counts, orient='index')\n",
    "df = df.sort_values(by=0, ascending=0)\n",
    "df1 = df[:20]\n",
    "plt.show(block=True)\n",
    "df1.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most common bigrams\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "report = allReports.lower()\n",
    "reportNLTK = nltk.Text(nltk.tokenize.word_tokenize(report))\n",
    "reportNLTK.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate some eeg reports for me by sequencing bigrams\n",
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "#print(type(text))\n",
    "corpus = nltk.corpus.reader.plaintext.PlaintextCorpusReader(\".\", \"allReports.txt\")\n",
    "words=corpus.words()\n",
    "#get rid of stop words\n",
    "#filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "#words = filtered_words\n",
    "\n",
    "#print(type(words))\n",
    "bigrams = nltk.bigrams(words)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "generate_model(cfd, 'seizure')\n",
    "#filtered_words = [word for word in word_list if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_model(cfd, 'epileptic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'slow')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'discharge')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'periodic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'impression')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'background')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'infantile')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'neonatal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'seizure')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'seizures')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'This')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'electrographic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'indicates')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'represent')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'generalized')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'slowing')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'focal')\n",
    "print(\"\")\n",
    "generate_model(cfd, '3')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'hz')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'second')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'button')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'cry')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'clonic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'tonic')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'partial')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'sleep')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'vertex')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'temporal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'frontal')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'occipital')\n",
    "print(\"\")\n",
    "#generate_model(cfd, 'hypsarrythmia')\n",
    "#print(\"\")\n",
    "generate_model(cfd, 'alpha')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'beta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'delta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'theta')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'sudden')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'waxing')\n",
    "print(\"\")\n",
    "generate_model(cfd, 'waning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fraction of words in text that are not in the stopword list\n",
    "def content_fraction(text):\n",
    "     stopwords = nltk.corpus.stopwords.words('english')\n",
    "     content = [w for w in text if w.lower() not in stopwords]\n",
    "     return len(content) / len(text)\n",
    "\n",
    "content_fraction(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tutorial example of classification\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000] \n",
    "\n",
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tutorial continued\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tutorial\n",
    "#what does it containt \n",
    "movie_reviews.categories()\n",
    "movie_reviews.fileids('neg')\n",
    "movie_reviews.categories()\n",
    "i = 1\n",
    "for doc in documents:\n",
    "    print(str(i) + \": \"+ doc[1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "[(name, 'female') for name in names.words('female.txt')])\n",
    "import random\n",
    "random.shuffle(labeled_names)\n",
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features\n",
    "\n",
    "featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
    "print(featuresets[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exclude = set(string.punctuation)\n",
    "print(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the most common word associated with the interpretation result (abnormal versus normal)\n",
    "#should take 10 mins\n",
    "import random\n",
    "import string\n",
    "documents = list()\n",
    "notebody = list()\n",
    "result = list()\n",
    "exclude = set(string.punctuation)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "with open(\"SHC_ALL_NOTES_IMPRESS.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        notebody = line['note'].lower()\n",
    "        notebodyStemmed = tokenize_and_stem(notebody)        \n",
    "        filtered_words = [word for word in notebodyStemmed if word not in stopwords]\n",
    "\n",
    "        #notebody = ''.join(ch for ch in notebody if ch not in exclude)\n",
    "        #notebody = notebody.split() \n",
    "        #notebody = [word for word in notebody if word not in stopwords.words('english')]\n",
    "        category = line['impression']\n",
    "        if (category == 'abnormal' or category == 'normal'):\n",
    "            documents.append((list(filtered_words), category))\n",
    "\n",
    "#documents[1]\n",
    "        # print(desc)\n",
    "        # now manipulate the note body\n",
    "\n",
    "random.shuffle(documents)\n",
    "#documents[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "i = 1\n",
    "for doc in documents:\n",
    "    print(doc[0])\n",
    "    i += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing a list of the 2000 most frequent words in the overall corpus\n",
    "import nltk\n",
    "words = list()\n",
    "for doc in documents:\n",
    "    words += doc[0] \n",
    "    \n",
    "#print(words)    \n",
    "all_words = nltk.FreqDist(w for w in words)\n",
    "word_features = list(all_words)[:4000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "for (d,c) in documents:\n",
    "    print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ccount = int (len(featuresets) / 2) - 1\n",
    "#take half of the total as the training set and the other half as a test set\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[ccount:], featuresets[:ccount]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)) \n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (d) in documents:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in order to load a corpus, reports must be in separate files // don't need to run this again\n",
    "\n",
    "capture_eegno = r'(?:DATE OF SERVICE:|STUDY DATE:|DATE EEG:|Date:|Service Date:|Study date|Study dates|DATE OF EEG:|start|T:|test dates:|Date of study:|Exam date|exam date:)\\s*(?P<eegno>[\\d/-]+)\\s*'\n",
    "capture_eegno1 = r'(?:Date:|Study date |Study date:)\\s*(?P<eegno>(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)\\s*[,\\s\\d/-]+)(?:[a-z]+)'\n",
    "capture_eegno2 = r'(?<!DOB:  )(?P<eegno>[\\d]+/[\\d]+/[\\d]+)' #DOB:  10/04/1993   \n",
    "eegDateRange = r'(?P<eegno>[\\d/]+-[\\d/]+)'\n",
    "eegDateStrict = r'(?P<eegno>[\\d]+/[\\d]+/[\\d]+)'\n",
    "#capture_eegno = r'(?P<eegno>[0-9]+)\\s*'\n",
    "re_eegno = re.compile(capture_eegno, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno1 = re.compile(capture_eegno1, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno2 = re.compile(capture_eegno2, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateRange = re.compile(eegDateRange, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateStrict = re.compile(eegDateStrict, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "\n",
    "out_file = open(\"lpch_eeg_reports_interp_date_impression.csv\",'w')\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    outfieldnames = reader.fieldnames\n",
    "    outfieldnames.append('date')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='***')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for line in reader:\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        print(str(i) + \": \", end='')\n",
    "        #print(str(i) + \": \" + str(type(m)), end='')\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            #print(eeg_no)\n",
    "        else:\n",
    "            m = re_eegno1.search(line['note'])\n",
    "            if m:\n",
    "                eeg_no = m.group('eegno')\n",
    "                #print(eeg_no)\n",
    "            else:\n",
    "                #print(\"!!!!!!!!!!!\")\n",
    "                for match in re.finditer(capture_eegno2,line['note']):\n",
    "                    eeg_no = match.group('eegno')\n",
    "                    \n",
    "                #m = re_eegno2.search(line['note'])\n",
    "                #if m:\n",
    "                #    eeg_no = m.group('eegno')\n",
    "                #print(eeg_no +\"???????\")\n",
    "\n",
    "        #if len(eeg_no) > 12:\n",
    "        #    m = eegDateRange.search(eeg_no)            \n",
    "        #    if m:\n",
    "        #        print(\"bad\")\n",
    "        #        m = eegDateStrict.search(eeg_no)\n",
    "        #        m = eegDateStrict.search(eeg_no)\n",
    "        #        if m:\n",
    "        #            eeg_no = m.group('eegno')\n",
    "        if ((len(eeg_no) < 5) | (len(eeg_no) > 12)):\n",
    "            #print(\"possibly bad: \" + eeg_no)\n",
    "            for match in re.finditer(capture_eegno2,line['note']):\n",
    "                eeg_no = match.group('eegno')\n",
    "        line['date'] = eeg_no \n",
    "        writer.writerow(line)\n",
    "        \n",
    "        #print(eeg_no)    \n",
    "        #i += 1\n",
    "        #if (i>600):\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capture_eegno = r'(?:EEG TYPE:)\\s*(?P<eegno>.+?)(?:history|report|clinical|condition|patient|location)'\n",
    "capture_eegno1 = r'(?:Date:|Study date |Study date:)\\s*(?P<eegno>(?:Jan|Feb|Mar|Apr|May|Jun|July|Aug|Sep|Oct|Nov|Dec)\\s*[,\\s\\d/-]+)(?:[a-z]+)'\n",
    "capture_eegno2 = r'(?<!DOB:  )(?P<eegno>[\\d]+/[\\d]+/[\\d]+)' #DOB:  10/04/1993   \n",
    "eegDateRange = r'(?P<eegno>[\\d/]+-[\\d/]+)'\n",
    "eegDateStrict = r'(?P<eegno>[\\d]+/[\\d]+/[\\d]+)'\n",
    "#capture_eegno = r'(?P<eegno>[0-9]+)\\s*'\n",
    "re_eegno = re.compile(capture_eegno, re.DOTALL|re.IGNORECASE)\n",
    "re_eegno1 = re.compile(capture_eegno1, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "re_eegno2 = re.compile(capture_eegno2, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateRange = re.compile(eegDateRange, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "eegDateStrict = re.compile(eegDateStrict, re.DOTALL|re.MULTILINE|re.IGNORECASE)\n",
    "\n",
    "out_file = open(\"lpch_eeg_reports_interp_date_impression_type.csv\",'w')\n",
    "\n",
    "def detectType(eeg_no):\n",
    "    if re.search(\"ambulatory\", eeg_no, re.IGNORECASE):\n",
    "        return \"amb\"\n",
    "    if re.search(\"routine\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"routine\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"portable\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"spot\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"out\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"sleep\", eeg_no, re.IGNORECASE):\n",
    "        return \"spot\"\n",
    "    if re.search(\"continuous\", eeg_no, re.IGNORECASE):\n",
    "        return \"ltm\"\n",
    "    if re.search(\"video\", eeg_no, re.IGNORECASE):\n",
    "        return \"ltm\"\n",
    "    if re.search(\"intraoperative\", eeg_no, re.IGNORECASE):\n",
    "        return \"iom\"\n",
    "    if re.search(\"electrocort\", eeg_no, re.IGNORECASE):\n",
    "        return \"iom\"\n",
    "    if re.search(\"amplitude\", eeg_no, re.IGNORECASE):\n",
    "        return \"aeeg\"\n",
    "    if re.search(\"aeeg\", eeg_no, re.IGNORECASE):\n",
    "        return \"aeeg\"\n",
    "    return \"\"\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_date_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    outfieldnames = reader.fieldnames\n",
    "    outfieldnames.append('type')\n",
    "    writer = csv.DictWriter(out_file, fieldnames=outfieldnames, restval='***')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for line in reader:\n",
    "        eeg_no=\"\"\n",
    "        m = re_eegno.search(line['note'])\n",
    "        print(str(i) + \": \", end='')\n",
    "        i += 1\n",
    "        #print(str(i) + \": \" + str(type(m)), end='')\n",
    "        if m:\n",
    "            eeg_no = m.group('eegno')\n",
    "            eeg_no = detectType(eeg_no)\n",
    "            \n",
    "        if (len(eeg_no) == 0):\n",
    "            eeg_no = detectType(line['note'])\n",
    "\n",
    "        line['type'] = eeg_no\n",
    "        \n",
    "        writer.writerow(line)\n",
    "        #if (i>1556): \n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in order to load a corpus, reports must be in separate files\n",
    "from dateutil import parser\n",
    "\n",
    "i=2\n",
    "with open(\"lpch_eeg_reports_interp_date_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    \n",
    "    for line in reader:\n",
    "        date_str= line['date']\n",
    "        try:\n",
    "            dateObj = parser.parse(date_str)\n",
    "            #print(dateObj.date())\n",
    "            dateFile = (str(dateObj.date()))\n",
    "            f = open(\"reports/\" + dateFile + \".txt\", \"a+\")\n",
    "            f.write(line['note'])\n",
    "            f.close()\n",
    "        except:\n",
    "            print(\"Unexpected error:\", str(sys.exc_info()[0]) + date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "#fileids = os.listdir(\"reports\")\n",
    "corpus_root = 'reports'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:10])\n",
    "           for fileid in wordlists.fileids()\n",
    "           for w in wordlists.words(fileid)\n",
    "           for target in ['grda', 'gpd']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd.tabulate(conditions=['grda','gpd'], samples=range(10), cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see if there are natural clusters, unsupervised\n",
    "#ngram of word, each one is dimension of word, then \n",
    "#applied text analysis python machine learning approaches on slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try tagging \n",
    "\n",
    "textAll_tokenized = list()\n",
    "with open(\"lpch_eeg_reports_interp_impression.csv\") as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for line in reader:\n",
    "        #print(line['note'])\n",
    "        text_raw = line['note'].lower()\n",
    "        text_tokenized = nltk.tokenize.word_tokenize(text_raw)\n",
    "        textAll_tokenized += text_tokenized\n",
    "        #words = list(text_raw.split())\n",
    "        #print(words)\n",
    "        #break\n",
    "\n",
    "text = nltk.Text(textAll_tokenized)\n",
    "text_tagged = nltk.pos_tag(text)\n",
    "\n",
    "        #print(text[10:200])\n",
    "print(text.similar('alpha'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which of these tags are the most common\n",
    "\n",
    "Tag\tMeaning\tEnglish Examples\n",
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADP\tadposition\ton, of, at, with, by, into, under\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CONJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer, article\tthe, a, some, most, every, no, which\n",
    "NOUN\tnoun\tyear, home, costs, time, Africa\n",
    "NUM\tnumeral\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT\tparticle\tat, on, out, over per, that, up, with\n",
    "PRON\tpronoun\the, their, her, its, my, I, us\n",
    "VERB\tverb\tis, say, told, given, playing, would"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset('NN.*')\n",
    "nltk.help.upenn_tagset('JJ.*')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in text_tagged)\n",
    "tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tag_pairs = nltk.bigrams(text_tagged)\n",
    "#i = 0\n",
    "#for (a, b) in word_tag_pairs:\n",
    "#    print(b[1])\n",
    "#    i += 1\n",
    "#    if i>22:\n",
    "#        break\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'JJ']\n",
    "#print(noun_preceders)\n",
    "fdist = nltk.FreqDist(noun_preceders)\n",
    "[tag for (tag, _) in fdist.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what are teh most common verbs\n",
    "word_tag_fd = nltk.FreqDist(text_tagged)\n",
    "[wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most likely words for a given tag VBN = \n",
    "cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in text_tagged)\n",
    "list(cfd2['VBN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#find vbd (past tense) and VBN (past participle), find words which can be both and see surrounding text\n",
    "nltk.help.upenn_tagset('VB.*')\n",
    "cfd1 = nltk.ConditionalFreqDist(text_tagged)\n",
    "[w for w in cfd1.conditions() if 'VBD' in cfd1[w] and 'VBN' in cfd1[w]]\n",
    "idx1 = text_tagged.index(('localized', 'VBD'))\n",
    "print(text_tagged[idx1-4:idx1+1])\n",
    "idx2 = text_tagged.index(('localized', 'VBN'))\n",
    "print(text_tagged[idx2-4:idx2+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
